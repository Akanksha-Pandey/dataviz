```{r echo = FALSE, message = FALSE}
# run setup script
source("_common.R")

library(lubridate)
library(tidyr)
```

# Visualizing trends {#visualizing-trends}



## Smoothing

*A few sentences stock market intro.*

The market lost over 2400 points (~27%) in the first three months of the year, and then it slowly recovered for the remainder of the year (Figure \@ref(fig:dow-jones)). How can we visualize these longer-term trends while de-emphasizing the less important short-term fluctuations?

(ref:dow-jones) Daily closing values of the Dow Jones Industrial Average for the year 2009. Data source: Yahoo! Finance

```{r dow-jones, fig.asp = 0.5, fig.cap = '(ref:dow-jones)'}
dow_jones_industrial %>% filter(date >= ymd("2008/12/31") & date <= ymd("2010/01/10")) %>%
  ggplot(aes(date, close)) + 
  geom_line(color = "grey20", size = .5) + 
  scale_x_date(limits = c(ymd("2008-12-31"), ymd("2010-01-10")), expand = c(0, 0)) +
  xlab(NULL) + ylab("Dow Jones Industrial Average") +
  theme_dviz_grid(12) +
  theme(
    plot.margin = margin(3, 12, 3, 0)
  )
```

In the financial world, it is commonplace to visualize trends with moving averages. To calculate a moving average, we take a time window, say the first 20 days in the time series, calculate the average price over these 20 days, then move the time window by one day, so it now spans the 2nd to 21st day, calculate the average over these 20 days, move the time window again, and so on. The result is a new time series consisting of a sequence of averaged prices.

To plot the time series of moving averages, we need to decide which specific time point to associate with the average for each time window. Financial analysts often plot each average at the end of its respective time window. This choice results in a curve that lags the original data (Figure \@ref(fig:dow-jones-moving-ave)a), with more severe lags corresponding to larger averaging time windows. Statisticians, on the other hand, plot the average at the center of the time window, which results in a curve that overlays perfectly on the original data (Figure \@ref(fig:dow-jones-moving-ave)b).

(ref:dow-jones-moving-ave) Daily closing values of the Dow Jones Industrial Average for the year 2009, shown together with their 20-day, 50-day, and 100-day moving averages. (a) The moving averages are plotted at the end of the moving time windows. (b) The moving averages are plotted in the center of the moving time windows. Data source: Yahoo! Finance

```{r dow-jones-moving-ave, fig.asp = 1, fig.cap = '(ref:dow-jones-moving-ave)'}
p1 <- dow_jones_industrial %>% filter(date >= ymd("2008/12/31") & date <= ymd("2010/01/10")) %>%
  mutate(
    close_20d_ave = moving_ave(date, close, 20, center = FALSE),
    close_50d_ave = moving_ave(date, close, 50, center = FALSE),
    close_100d_ave = moving_ave(date, close, 100, center = FALSE)
  ) %>%
  ggplot(aes(date, close)) + 
  geom_line(color = "grey20", size = .25) +
  geom_line(aes(date, close_20d_ave, color = "20d"), size = 1, na.rm = TRUE) +
  geom_line(aes(date, close_50d_ave, color = "50d"), size = 1, na.rm = TRUE) +
  geom_line(aes(date, close_100d_ave, color = "100d"), size = 1, na.rm = TRUE) +
  scale_color_manual(
    values = c(
      `20d` = "#009e73",
      `50d` = "#d55e00",
      `100d` = "#0072b2"
    ),
    breaks = c("20d", "50d", "100d"),
    labels = c("20 day average", "50 day average", "100 day average"),
    name = NULL
  ) + 
  scale_x_date(
    limits = c(ymd("2008-12-31"), ymd("2010-01-10")), expand = c(0, 0),
    labels = NULL
  ) +
  xlab(NULL) + ylab("Dow Jones industrial average") +
  theme_dviz_grid(12) +
  theme(
    plot.margin = margin(3, 12, 3, 0),
    legend.position = c(1, 0),
    legend.justification = c(1, 0),
    legend.box.background = element_rect(fill = "white", color = NA),
    legend.box.margin = margin(6, 12, 0, 12),
    axis.ticks.x = element_blank()
  )

p2 <- dow_jones_industrial %>% filter(date >= ymd("2008/12/31") & date <= ymd("2010/01/10")) %>%
  mutate(
    close_20d_ave = moving_ave(date, close, 20, center = TRUE),
    close_50d_ave = moving_ave(date, close, 50, center = TRUE),
    close_100d_ave = moving_ave(date, close, 100, center = TRUE)
  ) %>%
  ggplot(aes(date, close)) + 
  geom_line(color = "grey20", size = .25) +
  geom_line(aes(date, close_20d_ave, color = "20d"), size = 1, na.rm = TRUE) +
  geom_line(aes(date, close_50d_ave, color = "50d"), size = 1, na.rm = TRUE) +
  geom_line(aes(date, close_100d_ave, color = "100d"), size = 1, na.rm = TRUE) +
  scale_color_manual(
    values = c(
      `20d` = "#009e73",
      `50d` = "#d55e00",
      `100d` = "#0072b2"
    ),
    breaks = c("20d", "50d", "100d"),
    labels = c("20 day average", "50 day average", "100 day average"),
    name = NULL,
    guide = "none"
  ) + 
  scale_x_date(limits = c(ymd("2008-12-31"), ymd("2010-01-10")), expand = c(0, 0)) +
  xlab(NULL) + ylab("Dow Jones Industrial Average") +
  theme_dviz_grid(12) +
  theme(
    plot.margin = margin(3, 12, 3, 0)
  )

plot_grid(p1, p2, ncol = 1, align = 'h', labels = "auto")
```

Regardless of whether we plot the smoothed time series with or without lag, we can see that the length of the time window over which we average sets the scale of the fluctuations that remain visible in the smoothed curve. The 20-day moving average only removes small, short-term spikes but otherwise follows the daily data closely. The 100-day moving average, on the other hand, removes even fairly substantial drops or spikes that play out over a time span of multiple weeks. For example, the massive drop to below 7000 points in the first quarter of 2009 is not visible in the 100-day moving average, which replaces it with a gentle curve that doesn't dip much below 8000 points (Figure \@ref(fig:dow-jones-moving-ave)). Similarly, the drop around July 2009 is completely invisible in the 100-day moving average.

The moving average is a fairly simplistic approach to smoothing, and it has some obvious limitations. First, the moving window approach results in a smoothed curve that is shorter than the original curve (Figure \@ref(fig:dow-jones-moving-ave)). Parts are missing at either the beginning or the end or both. And the more the time series is smoothed (i.e., the larger the averaging window), the shorter the smoothed curve. Second, even though a moving average with large averaging window can smooth over fairly substantial deviations from the overall trend, the resulting curve is not necessarily that smooth. It may display small scale bumps and wiggles despite the larger-scale smoothing that has been achieved (Figure \@ref(fig:dow-jones-moving-ave)). These wiggles are caused by individual data points that enter or exit the averaging window. Since all data points in the window are weighted equally, individual data points at the window boundaries can have visible impact on the average.

Statisticians have developed numerous approaches to smoothing that alleviate the downsides of moving averages. These approaches are much more complex and computationally costly, but they are readily available in modern statistical computing environments. One widely used method is LOESS (locally estimated scatterplot smoothing, @Cleveland1979), which fits low-degree polynomials to subsets of the data. Importantly, the points in the center of each subset are weighted more heavily than points at the boundaries, and this weighting scheme yields a much smoother result than we get from a weighted average (Figure \@ref(fig:dow-jones-loess)).

(ref:dow-jones-loess) Comparison of LOESS fit to 100-day moving average for the Dow Jones data of Figure \@ref(fig:dow-jones-moving-ave). The overall trend shown by the LOESS smooth is nearly identical to the 100-day moving average, but the LOESS curve is much smoother and it extends to the entire range of the data. Data source: Yahoo! Finance

```{r dow-jones-loess, fig.asp = 0.5, fig.cap = '(ref:dow-jones-loess)'}
# LOESS (locally estimated scatterplot smoothing) 

dow_jones_industrial %>% filter(date >= ymd("2008/12/31") & date <= ymd("2010/01/10")) %>%
  mutate(
    close_100d_ave = moving_ave(date, close, 100)
  ) %>%
  ggplot(aes(date, close)) + 
  geom_line(color = "grey20", size = .25) +
  geom_line(aes(date, close_100d_ave, color = "100d"), size = 1, na.rm = TRUE) +
  geom_smooth(aes(color = "smooth"), size = 1, na.rm = TRUE, se = FALSE) +
  scale_color_manual(
    values = c(
      `100d` = "#d55e00",
      smooth = "#0072b2"
    ),
    breaks = c("smooth", "100d"),
    labels = c("LOESS smoother", "100 day average"),
    name = NULL
  ) + 
  scale_x_date(limits = c(ymd("2008-12-31"), ymd("2010-01-10")), expand = c(0, 0)) +
  xlab(NULL) + ylab("Dow Jones Industrial Average") +
  theme_dviz_grid(12) +
  theme(
    legend.position = c(1, 0.48),
    legend.justification = c(1, 0.5),
    legend.box.background = element_rect(fill = "white", color = NA),
    legend.box.margin = margin(0, 12, 6, 12),
    plot.margin = margin(3, 12, 3, 0)
  )
```

Another advantage of LOESS is that it is not limited to time series. It can be applied to arbitrary scatter plots, as is apparent from its name, *locally estimated scatterplot smoothing*.  *continue here.*

(ref:tank-capacity-loess) Fuel-tank capacity versus price of 93 cars released for the 1993 model year. Each dot corresponds to one car. The solid line represents a LOESS smooth of the data. We see that fuel-tank capacity increases approximately linearly with price up to a price of approximately $20,000, and then it levels off. Data source: Robin H. Lock, St. Lawrence University

```{r tank-capacity-loess, fig.width = 5, fig.asp = 0.75, fig.cap='(ref:tank-capacity-loess)'}
cars93 <- MASS::Cars93

ggplot(cars93, aes(x = Price, y = Fuel.tank.capacity)) + 
  geom_point(color = "grey60") + 
  geom_smooth(se = FALSE, method = "loess", formula = y ~ x, color = "#0072B2") +
  scale_x_continuous(
    name = "price (USD)",
    breaks = c(20, 40, 60),
    labels = c("$20,000", "$40,000", "$60,000")
  ) +
  scale_y_continuous(name = "fuel tank capacity\n(US gallons)") +
  theme_minimal_grid()
```

Because LOESS requires the fitting of many separate regression models, it can be slow for large datasets, even on modern computing equipment. To rapidly smooth large datasets, we can instead use spline models. A spline is a piecewise polynomial function *continue.*



There are many different smoothing functions ... This is important, because most statistical packages will provide a default smoother, and you may never realize to what extent the results you see depend on the specific default choices made by your statistical software.

(ref:tank-capacity-smoothers) Different smoothing models display widely different behaviors near the boundaries of the data. (a) LOESS smoother, as in Figure \@ref(fig:tank-capacity-loess). (b) Cubic regression splines with 5 knots. (c) Thin-plate regression spline with 3 knots. (d)  Gaussian process spline with 6 knots. Data source: Robin H. Lock, St. Lawrence University

```{r tank-capacity-smoothers, fig.width=8.5, fig.asp = 0.75, fig.cap='(ref:tank-capacity-smoothers)'}
cars_base <- ggplot(cars93, aes(x = Price, y = Fuel.tank.capacity)) + geom_point(color = "grey60") + 
  scale_x_continuous(
    name = "price (USD)",
    breaks = c(20, 40, 60),
    labels = c("$20,000", "$40,000", "$60,000")
  ) +
  scale_y_continuous(name = "fuel tank capacity\n(US gallons)") +
  theme_minimal_grid(12)  
  
p1 <- cars_base + geom_smooth(se = FALSE, method = "loess", formula = y ~ x, color = "#0072B2")
p2 <- cars_base + geom_smooth(se = FALSE, method = "gam", formula = y ~ s(x, k = 5, bs = 'cr'), color = "#0072B2")
p3 <- cars_base + geom_smooth(se = FALSE, method = "gam", formula = y ~ s(x, k = 3), color = "#0072B2")
p4 <- cars_base + geom_smooth(se = FALSE, method = "gam", formula = y ~ s(x, k = 6, bs = 'gp'), color = "#0072B2")

plot_grid(
  p1, p2, p3, p4, align = 'hv',
  labels = 'auto'
)

# See also: https://altaf-ali.github.io/ggplot_tutorial/smoother.html
```

## Explicit models

(ref:tank-capacity-model) **Figure title.** The solid line corresponds to the formula $y = 68.20 - 46.37 x/(x - 1,560)$. Data source: Robin H. Lock, St. Lawrence University

```{r tank-capacity-model, fig.asp = 0.75, fig.cap = '(ref:tank-capacity-model)'}
fit.out <- nls(Fuel.tank.capacity ~ a*Price/(Price + b) + c, data = cars93, start = c(a = -45, b = -1, c = 70))
fit.df <- data.frame(Price = 7:62,
                     Fuel.tank.capacity = predict(fit.out, data.frame(Price = 7:62)))

ggplot(cars93, aes(x = Price, y = Fuel.tank.capacity)) + geom_point(color = "grey60") + 
  geom_line(data = fit.df, size = 1, color = "#0072B2") +
  scale_x_continuous(name = "price (USD)",
                     breaks = c(20, 40, 60),
                     labels = c("$20,000", "$40,000", "$60,000")) +
  scale_y_continuous(name = "fuel tank capacity\n(US gallons)") +
  theme_minimal_grid()
```


(ref:blue-jays-scatter-line) Head length versus body mass for 123 blue jays. The birds' sex is indicated by color. At the same body mass, male birds tend to have longer heads (and specifically, longer bills) than female birds. Data source: Keith Tarvin, Oberlin College

```{r blue-jays-scatter-line, fig.width = 5, fig.asp = 3/4, fig.cap='(ref:blue-jays-scatter-line)'}
ggplot(blue_jays, aes(Mass, Head, color = KnownSex, fill = KnownSex)) + 
  geom_point(pch = 21, color = "white", size = 2.5) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(name = "body mass (g)") +
  scale_y_continuous(name = "head length (mm)") +
  scale_fill_manual(
    values = c(F = "#D55E00B0", M = "#0072B2B0"),
    breaks = c("F", "M"),
    labels = c("female birds   ", "male birds"),
    name = NULL,
    guide = guide_legend(direction = "horizontal")) +
  scale_color_manual(
    values = c(F = "#D55E00", M = "#0072B2"),
    breaks = c("F", "M"),
    labels = c("female birds   ", "male birds"),
    name = NULL,
    guide = guide_legend(direction = "horizontal")) +
  theme_dviz_grid() +
  theme(
    legend.position = c(1, 0.01),
    legend.justification = c(1, 0),
    legend.spacing.x = unit(2, "pt"),
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(20, "pt")
  )
```

(ref:biorxiv-logscale) Monthly submissions to the preprint server bioRxiv. **comment on the log scale.** Data source: Jordan Anaya, http://www.prepubmed.org/

```{r biorxiv-logscale, fig.cap = '(ref:biorxiv-logscale)'}
preprint_growth %>% filter(archive == "bioRxiv") %>%
  filter(count > 0) -> biorxiv_growth

ggplot(biorxiv_growth, aes(date, count)) + 
  geom_line(color = "#0072B2", size = .75) +
  scale_y_log10(limits = c(30, 1600),
                breaks = c(10*(3:9), 100*(1:9), 1000*(1:2)),
                labels = c("", "", "50", "", "", "", "",
                           "100", "", "", "", "500", "", "", "", "",
                           "1000", ""), expand = c(0, 0),
                name = "preprints / month") + 
  scale_x_date(name = NULL) +
  theme_dviz_open() +
  theme(plot.margin = margin(7, 7, 3, 0))
```


## Detrending and time-series decomposition

(ref:hpi-trends) Freddie Mac House Price Index from 1980 through 2017, for four selected states (California, Nevada, Texas, and West Virginia). The House Price Index is a unitless number that tracks relative house prices in the chosen geographic region over time. The index is scaled arbitrarily such that it equals 100 in December of the year 2000. The blue lines show the monthly fluctuations in the index and the straight gray lines show the long-term price trends in the respective states. Data source: Freddie Mac House Prices Index

```{r hpi-trends, fig.cap = '(ref:hpi-trends)'}
hpi_trends <- house_prices %>%
  filter(year(date) >= 1980) %>%
  filter(state %in% c("California", "Nevada", "West Virginia", "Texas")) %>%
  mutate(
    date_numeric = as.numeric(date),
    hpi = house_price_index,
    log_hpi = log(hpi)
  ) %>%
  group_by(state) %>%
  mutate(
    hpi_trend = {
      coefs <- coef(lm(log_hpi ~ date_numeric))
      exp(coefs[1] + coefs[2]*date_numeric)
    },
    hpi_detrended = hpi/hpi_trend
  )

ggplot(hpi_trends, aes(date, hpi)) +
  geom_line(aes(y = hpi_trend), color = "grey50", size = 0.4) +
  geom_line(color = "#0072B2") +
  scale_x_date(name = NULL) +
  scale_y_log10(name = "House Price Index (Dec. 2000 = 100)") +
  facet_wrap(~state, scales = "free_x") +
  theme_dviz_hgrid(12, rel_small = 1) +
  theme(
    strip.background = element_rect(fill = "grey90"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )
    
```

While one could arguably draw the same conclusions from the original data (Figure \@ref(fig:hpi-trends)), detrending helps to emphasize the unexpected movements in a time series. For example, in the original time series, the decline in home prices in California from 1990 to about 1998 looks modest (Figure \@ref(fig:hpi-trends)). However, during that same time period, on the basis of the long-term trend we would have expected prices to rise. Relative to the expected rise the drop in prices was substantial, amounting to 25% at the lowest point (Figure \@ref(fig:hpi-detrended)).



(ref:hpi-detrended) Detrended version of the Freddie Mac House Price Index shown in Figure \@ref(fig:hpi-trends). The detrended index was calculated by dividing the actual index (blue lines in Figure \@ref(fig:hpi-trends)) by the expected value based on the long-term trend (straight gray lines in Figure \@ref(fig:hpi-trends)). This visualization shows that California experienced two housing bubbles, around 1990 and in the mid-2000s, identifiable from a rapid rise and subsequent decline in the actual housing prices relative to what would have been expected from the long-term trend. Similarly, Nevada experienced one housing bubble, in the mid-2000s, and neither Texas nor West Virginia experienced much of a bubble at all. Data source: Freddie Mac House Prices Index

```{r hpi-detrended, fig.cap = '(ref:hpi-detrended)'}
ggplot(hpi_trends, aes(date, hpi_detrended)) +
  geom_hline(yintercept = 1, color = "grey50", size = 0.4) +
  geom_line(color = "#0072B2") +
  scale_x_date(name = NULL) +
  scale_y_log10(
    name = "House Price Index (detrended)",
    breaks = c(0.752, 1, 1.33, 1.77),
    labels = c("0.75", "1.00", "1.33", "1.77")
  ) +
  facet_wrap(~state, scales = "free_x") +
  theme_dviz_hgrid(12, rel_small = 1) +
  theme(
    strip.background = element_rect(fill = "grey90"),
    axis.line.x = element_line(color = "grey50"),
    axis.ticks.x = element_line(color = "grey50"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_text(margin = margin(0, 0, 0, 0))
  )
 
 
```


In general, there are three different processes that can lead to deviations from the long-term trend in a time series. First, there is random noise, which causes small, erratic movements up and down. This noise is visible in all the time series shown in this chapter, but maybe the most clearly in Figure \@ref(fig:biorxiv-logscale). Second, there can be unique external events that leave their mark in the time series, such as the distinct housing bubbles seen in Figure \@ref(fig:hpi-trends). Third, there can be cyclical variations. For example, temperatures show daily cyclical variations, with the highest temperatures usually reached in the early afternoon and the lowest temperatures in the early morning. Temperatures also show yearly cyclical variations. They tend to be rise in the spring, reach their maximum in the summer, and then decline in fall and reach their minimum in the winter (Figure \@ref(fig:temperature-normals-Houston)).

When a time series consists of a combination of multiple processes, we may want to decompose the series into distinct components representing these processes. To demonstrate this concept, I will decompose the Keeling curve, which shows changes in CO<sub>2</sub> abundance over time (Figure \@ref(fig:keeling-curve)). CO<sub>2</sub> is measured in parts per million (ppm). We see a long-term increase in CO<sub>2</sub> abundance that is slightly faster than linear, from below 325 ppm in the 1960s to above 400 in the second decade of the 21st century (Figure \@ref(fig:keeling-curve)). CO<sub>2</sub> abundance also fluctuates anually, following a fairly regular pattern. These fluctuation are driven by plant growth in the northern hemisphere. Plants consume CO<sub>2</sub> during photosynthesis. Because most of the globe's land masses are located in the northern hemisphere, and plant growth is most active in the spring and summer, we see an annual global decline in atmospheric CO<sub>2</sub> that coincides with the summer in the northern hemisphere.

(ref:keeling-curve) The Keeling curve. The Keeling curve shows the change of CO<sub>2</sub> abundance in the atmosphere over time. Since 1958, CO<sub>2</sub> abundance has been continuously monitored at the Mauna Loa Observatory in Hawaii, initially under the direction of Charles Keeling. Shown here are monthly average CO<sub>2</sub> readings, expressed in parts per million (ppm). The CO<sub>2</sub> readings fluctuate anually with the seasons but show a consistent long-term trend of increase. Data source: Dr. Pieter Tans, NOAA/ESRL, and Dr. Ralph Keeling, Scripps Institution of Oceanography

```{r keeling-curve, fig.cap = '(ref:keeling-curve)'}
# use complete years only
ggplot(CO2, aes(date_dec, co2_interp)) +
  geom_line(color = "#0072B2") +
  scale_y_continuous(
    limits = c(295, 418),
    breaks = c(300, 325, 350, 375, 400),
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(1958, 2019),
    name = NULL,
    breaks = c(1960, 1970, 1980, 1990, 2000, 2010),
    labels = c("", "1970", "", "1990", "", "2010"),
    expand = c(0, 0)
  ) +
  theme_dviz_grid()
```

We can decompose the Keeling curve into its long-term trend, seasonal fluctuations, and remainder (Figure \@ref(fig:keeling-curve-decomposition)). The specific method I am using here is called STL (Seasonal decomposition of Time series by LOESS, @Cleveland_et_al_1990), but there are many other methods that achieve similar goals. The decomposition shows that over the last three decades, CO<sub>2</sub> abundance has increased by over 50 ppm. By comparison, seasonal fluctuations amount to less than 8 ppm (they never cause an increase or a decrease in more than 4 ppm relative to the long-term trend), and the remainder amounts to less than 1.6 ppm (Figure \@ref(fig:keeling-curve-decomposition)). The remainder is the difference between the actual readings and the sum of the long-term trend and the seasonal fluctuations, and here it corresponds to random noise in the monthly CO<sub>2</sub> readings. More generally, however, the remainder could also capture unique external events. For example, if a massive volcano erruption released substantial amounts of CO<sub>2</sub>, such an event might be visible as a sudden spike in the remainder. Figure \@ref(fig:keeling-curve-decomposition) shows that no such unique external events have had a major effect on the Keeling curve in recent decades. 

(ref:keeling-curve-decomposition) Time-series decomposition of the Keeling curve, showing the montly average (as in Figure \@ref(fig:keeling-curve)), the long-term trend, seasonal fluctuations, and remainder. The remainder is the difference between the actual readings and the sum of the long-term trend and the seasonal fluctuations, and it represents random noise. I have zoomed into the most recent 30 years of data to more clearly show the shape of the annual fluctuations. Data source: Dr. Pieter Tans, NOAA/ESRL, and Dr. Ralph Keeling, Scripps Institution of Oceanography

```{r keeling-curve-decomposition, fig.asp = 0.9, fig.cap = '(ref:keeling-curve-decomposition)'}
# use complete years only
CO2_complete <- filter(CO2, year >= 1959, year < 2018)
# convert to time series object
CO2_ts <- ts(data = CO2_complete$co2_interp, start = 1959, end = c(2017, 12), frequency = 12)
# detrend via STL method
CO2_stl <- stl(CO2_ts, s.window = 7)
CO2_detrended <- mutate(
  CO2_complete,
  `monthly average` = co2_interp,
  `seasonal fluctuations` = t(CO2_stl$time.series)[1, ],
  `longterm trend` = t(CO2_stl$time.series)[2, ],
  remainder = t(CO2_stl$time.series)[3, ]
)

facet_labels <- c("monthly average", "longterm trend", "seasonal fluctuations", "remainder")

CO2_detrended %>%
  select(date_dec, `monthly average`, `seasonal fluctuations`, `longterm trend`, remainder) %>%
  gather(variable, value, -date_dec) %>%
  mutate(variable = factor(variable, levels = facet_labels)) %>%
  filter(date_dec >= 1989) %>%
  ggplot(aes(date_dec, value)) +
  geom_line(color = "#0072B2") +
  geom_point(
    data = data.frame(
      variable = factor(
        rep(facet_labels, each = 2),
        levels = facet_labels
      ),
      x = 1990,
      #y = c(295, 424, 295, 424, -4.1, 4.3, -.81, .85)
      y = c(324, 419, 324, 419, -4.1, 4.3, -.81, .85)
    ),
    aes(x, y),
    color = NA,
    na.rm = TRUE
  ) +
  scale_y_continuous(
    name = parse(text = "`CO`[2]*` concentration (ppm)`"),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    limits = c(1989, 2018.2),
    name = NULL,
    breaks = c(1990, 1995, 2000, 2005, 2010, 2015),
    labels = c("1990", "", "2000", "", "2010", ""),
    expand = c(0, 0)
  ) +
  facet_wrap(facets = vars(variable), ncol = 1, scales = "free") +
  theme_dviz_grid(12) +
  theme(
    plot.margin = margin(3, 0, 3, 1),
    strip.text = element_text(size = 11)
  )
```

```{r}
# links with useful articles about detrending:
# https://anomaly.io/seasonal-trend-decomposition-in-r/
# http://r-statistics.co/Time-Series-Analysis-With-R.html
# https://econometricswithr.wordpress.com/2015/10/27/extracting-business-cycles-from-raw-data-in-r/
```
