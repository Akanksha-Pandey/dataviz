[
["index.html", "Fundamentals of data visualization Preface", " Fundamentals of data visualization Claus O. Wilke Preface This book is meant as a guide for making figures that look professional and are publication-ready. It has grown out of my experience of having to repeatedly give my trainees the same kinds of advice—use larger fonts, pay attention to the aspect ratio of your figure, use solid colors rather than outlines, and so on. Now, I can just aks them to read the appropriate chapters in this book. The entire book was written in R Markdown, using RStudio as my text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub, at https://github.com/clauswilke/dataviz. If you would like to fix typos or other issues, feel free to send me pull requests through GitHub. In your commit message, please add the sentence “I assign the copyright of this contribution to Claus O. Wilke,” so that I can maintain the option of publishing this book in other forms. For comments, questions, or requests for additional chapters, please open an issue on GitHub. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["introduction.html", "Introduction Thoughts on graphing software and figure-preparation pipelines Good, bad, and ugly figures Acknowledgments", " Introduction If you are a scientist, an analyst, a consultant, or anybody else who has to prepare technical documents or reports, one of the most important skills you need to have is the ability to make compelling data visualizations, generally in the form of figures. Figures will typically carry the weight of your arguments. They need to be clear, attractive, and convincing. The difference between good and bad figures can be the difference between a highly influential or a obscure paper, a grant or contract won or lost, a job interview gone well or poorly. And yet, there are surprisingly few resources to teach you how to make compelling data visualizations. There are no college courses on this topic, and no extensive collection of books you can read.1 Tutorials for plotting sofware typically focus on how to achieve specific visual effects rather than explaining why certain choices are preferred and others not. In your day-to-day work, you are simply expected to know how to make good figures, and if you’re lucky you have a patient adviser who teaches you a few tricks as you’re writing your first scientific papers. In the context of writing, experienced editors talk about “ear”, the ability to hear (internally, as you read a piece of prose) whether the writing is any good. I think that when it comes to figures and other visualizations, we similarly need “eye”, the ability to look at a figure and see whether it is balanced, clear, and compelling. And just as is the case with writing, the ability to see whether a figure works or not can be learned. Having eye means primarily that you are aware of a larger collection of simple rules and principles of good visualization, and that you pay attention to little details that other people might not. In my experience, again just as in writing, you don’t develop eye by reading a book over the weekend. It is a lifelong process, and concepts that are too complex or too subtle for you today may make much more sense five years from now. I can say for myself that I continue to evolve in my understanding of figure preparation. I routinely try to expose myself to new approaches, and I pay attention to the visual and design choices others make in their figures. I’m also open to change my mind. I might today consider a given figure great, but next month I might find a reason to criticize it. So with this in mind, please don’t take anything I say as gospel. Think critically about my reasoning for certain choices and decide whether you want to adopt them or not. This book is written as a series of independent blog posts, and there is no need to read it cover to cover. Feel free to skip around, to pick out a specific section that you’re interested in at the moment, or one that covers a specific design choice you’re pondering. In fact, I think you will get the most out of this book if you don’t read it all at once, but rather read it piecemeal over longer stretches of time, try to apply just a few concepts from the book in your figuremaking, and come back to read about other concepts or re-read concepts you learned about a while back. You may find that the same chapter tells you different things if you re-read it after a few months of time have passed. Even though all the figures in this book were made with R and ggplot2, I do not see this as an R book. I am talking about general principles of figure preparation. The software used to make the figures is incidental. You can use any plotting software you want to generate the kinds of figures I’m showing here, even if ggplot2 and similar packages make many of the techniques I’m using much simpler than other plotting libraries. Importantly, because this is not an R book, I do not discuss code or programming techniques anywhere in this book. I want you to focus on the concepts and the figures, not on the code. If you are curious how any of the figures were made, you can check out the book’s source code at its GitHub repository, https://github.com/clauswilke/professional_figures. Thoughts on graphing software and figure-preparation pipelines I have over two decades of experience preparing figures for scientific publications and have made thousands of figures. If there is one constant over these two decades, it’s the change in figure preparation pipelines. Every few years, a new plotting library is developed or a new paradigm arises, and large groups of scientists switch over to the hot new toolkit. I have made figures using gnuplot, Xfig, Mathematica, Matlab, matplotlib in python, base R, ggplot2 in R, and possibly others I can’t currently remember. My current preferred approach is ggplot2 in R, but I’m under no illusion that I’ll continue using it until I retire. This constant change in software platforms is one of the key reasons why this book is not a programming book and why I have left out all code examples. I want this book to be useful to you regardless of which software you use, and I want it to remain valuable even once everybody has moved on from ggplot2 and uses the next new thing. I realize that this choice may be frustrating to some ggplot2 uses who would like to know how I made a given figure. To them I say, read the source code of the book. It is available. Also, in the future I may release a supplementary document focused just on the code. One thing I have learned over the years is that automation is your friend. I think figures should be autogenerated as part of the data analysis pipeline (which should also be automated), and they should come out of the pipeline ready to be sent to the printer, no manual post-processing needed. I see a lot of trainees autogenerate rough drafts of their figures, which they then import into Illustrator for sprucing up. There are several reasons why this is a bad idea. First, the moment you manually edit a figure, your final figure becomes irreproducible. A third party cannot generate the exact same figure you did. While this may not matter much if all you did was change the font of the axis labels, the lines are blurry, and it’s easy to cross over into territory where things are less clear cut. As an example, let’s say to manually replaced cryptic labels with more readable ones. A third party may not be able to verify that the label replacement was appropriate. Second, if you add a lot of manual post-processing to your figure-preparation pipeline then you will be more reluctant to make any changes or redo your work. Thus, you may ignore reasonable requests for change made by collaborators or colleagues, or you may be tempted to re-use an old figure even though you actually regenerated all the data. These are not made-up examples. I’ve seen all of them play out with real people and real papers. Third, you may yourself forget what exactly you did to prepare a given figure, or you may not be able to generate a future figure on new data that exactly visually matches your earlier figure. For all the above reasons, interactive plot programs are a bad idea. They inherently force you to manually prepare your figures. In fact, it’s probably better to auto-generate a figure draft and spruce it up in Illustrator than make the entire figure by hand in some interactive plot program. Please be aware that Excel is an interactive plot program as well and is not recommended for figure preparation (or data analysis). One critical component in a book on data visualization is feasibility of the proposed visualizations. It’s nice to invent some elegant new way of visualization, but if nobody can easily generate figures using this visualization then there isn’t much use to it. For example, when Tufte first proposed sparklines nobody had an easy way of making them. While we need visionaries who move the world foward by pushing the envelope of what’s possible, I envision this book to be practical and directly applicable to working scientists preparing figures for their publications. Therefore, every visualization I propose in the subsequent chapters can be generated with a few lines of R code via ggplot2 and readily available extension packages. In fact, every figure in this book was autogenerated exactly as shown, using R and ggplot2. Good, bad, and ugly figures Throughout this book, I am showing many different versions of the same figures, some as examples of how make a good visualization and some as examples of how not to. To provide a simple visual guideline of which examples should be emulated and which should be avoided, I am grading each figure on a three-point scale, “good”, “ugly”, and “bad”: good—A figure that looks nice and could be printed as is. Note that among good figures, there will still be differences in quality, and some good figures will be better than others. ugly—A figure that has one ore more shortcomings that should be remediated, or a figure that I personally don’t find aesthetically pleasing. bad—A figure that has one or more objective flaws which make it unsuitable for publication. I generally provide my rationale for specific ratings, but some are a matter of taste. In particular when it comes to the difference between “ugly” and “good”, reasonable people may disagree. I encourage you to develop your own eye and to critically evaluate my choices. Acknowledgments This project would not have been possible without the fantastic work the RStudio team has put into turning the R universe into a first-rate publishing platform. In particular, I have to thank Hadley Wickham for creating ggplot2, the plotting software that was used to make all the figures throughout this book. I would also like to thank Yihui Xie for creating the knitr package, R Markdown, and bookdown. I don’t think I would have started this project without these tools ready to go. Even though I have over 20 years of experience authoring documents in LaTeX, and I have written and continue to write almost all my scientific papers using that platform, the idea of writing a LaTeX document that contains hundreds of figures, having to maintain the R code for all these figures separately, and having to manually organize storage and naming of all the figure files makes me cringe. By contrast, writing self-contained R Markdown files is fun, and it’s easy to collect material and gain momentum. One notable exception is the works of Edward Tufte. His books and seminars are excellent, and much of what I do and say has been inspired by him.↩ "],
["visualizing-data-mapping-data-onto-aesthetics.html", "1 Visualizing data: mapping data onto aesthetics", " 1 Visualizing data: mapping data onto aesthetics "],
["choosing-colorblind-friendly-color-scales.html", "2 Choosing colorblind-friendly color scales 2.1 Qualitative color scales 2.2 Directional color scales", " 2 Choosing colorblind-friendly color scales When coloring elements in your figures, you need keep in mind that a good proportion of your readers may have color-vision deficiency (i.e., are colorblind) and may not be able to distinguish the colors that look so clearly different to you. You could address this problem by preparing all figures in grayscale, but those figures would look drab and boring to everybody, even those with impaired color vision. Alternatively, you can (and should) employ redundant coding, discussed in more detail in Chapter 3. However, independent of whether or not you use redundant coding, you should consider using a color scale whose colors are distinguishable even for people with color-vision deficiency. In choosing your color scale, remember that people with impaired color vision are not literally unable to see any colors. Instead, they will typically have difficulty to distinguish certain types of colors, for example red and green (red–green colorblindness) or blue and green (blue-yellow colorblindness). The key to making a colorblind-friendly color scale is to (i) choose colors with different levels of brightness and saturation, and (ii) choose color combinations that tend to look dissimilar even if color vision is partially impaired. Unless you’re an expert in color theory, go with an existing color scheme instead of designing your own. 2.1 Qualitative color scales We frequently need to color discrete items that do not have an inherent order, such as different countries on a map or different manufacturers of a certain product. In those cases, we use qualitative color scales, which are color scales with a finite set of specific colors that are chosen to look as different from each other as possible. My preferred color scale of this type, which I use extensively throughout this book, was developed specifically to work well for all the major types of color-vision deficiency (Okabe and Ito 2008): By providing eight different colors, this palette works for nearly any scenario with discrete colors. You should probably not color-code more than eight different items in a plot anyways. A variant of the palette replaces black with gray, if you don’t like to see completely black visual elements: In these palettes, the alphanumeric codes represent the colors in RGB space, encoded as hexadecimals. In many plot libraries and image-manipulation programs, you can just enter these codes directly. If your software does not take hexadecimals directly, you can also use Table 2.1. Table 2.1: Colorblind-friendly color scale, developed by Okabe and Ito (2008). Name Hex code Hue C, M, Y, K (%) R, G, B (0-255) R, G, B (%) black #000000 - 0, 0, 0, 100 0, 0, 0 0, 0, 0 gray #999999 - 0, 0, 0, 60 153, 153, 153 60, 60, 60 orange #E69F00 41° 0, 50, 100, 0 230, 159, 0 90, 60, 0 sky blue #56B4E9 202° 80, 0, 0, 0 86, 180, 233 35, 70, 90 bluish green #009E73 164° 97, 0, 75, 0 0, 158, 115 0, 60, 50 yellow #F0E442 56° 10, 5, 90, 0 240, 228, 66 95, 90, 25 blue #0072B2 202° 100, 50, 0, 0 0, 114, 178 0, 45, 70 vermilion #D55E00 27° 0, 80, 100, 0 213, 94, 0 80, 40, 0 reddish purple #CC79A7 326° 10, 70, 0, 0 204, 121, 167 80, 60, 70 2.2 Directional color scales A second common use of color is to indicate intensity of some sort, for example temperature or speed. In those cases, we need a directional color scale that clearly indicates (i) which values are larger or smaller than which other ones, and (ii) how distant two specific values are from each other. The second point is particularly important, because it implies that the color scale needs to vary uniformly across its entire range. There cannot be parts of the scale where color changes more quickly than in other parts. Designing such a color scale, in particular while respecting the additional requirement that they work for viewers with impaired color vision, can be surprisingly difficult. However, recent research into color scales has solved these issues and produced some very nice color scales (Smith and van der Walt 2015). In brief, all these color scales satisfy the following conditions. First, brightness varies continuosly from dark to light, such that the color scale works even when reproduced in grayscale. Second, the major color axis changes from blue to yellow, because the alternative, red to green, would not work for the most common forms of color-vision deficiency, red-green blindness. Third, the blue colors appear on the dark side of the scale and the yellow colors on the light side, because gradients from dark yellow to bright blue look unnatural. With these constraints, there is only one major choice remaining, which is whether to transition from blue to yellow through green or through red. The following four color scales, which are now available for both python and R graphics and are becoming increasingly popular, meet all these requirements. You can see examples using these scales throughout this book, for example in the chapter on redundant coding (Chapter 3). References "],
["redundant-coding.html", "3 Redundant coding", " 3 Redundant coding Need some intro text here about the concept of redundant coding, showing the same information with multiple visua elements that reinforce each other. There are two problems with this figure: 1. All points have the same shape. 2. The two colors that are more similar to each other (blue and green) are used for the data points that are the most intermingled (virginica and versicolor). We address these two problems by using different shapes and switching out the colors. Notice how it has become easier to distinguish virginica and versicolor. Also, now that we use different shapes, we see that some virginica points fully overlap with some versicolor points. (These points appear as little stars, because the square representing versicolor is shown underneath the diamond representing virginica.) To learn more about ways to handle overlapping points, see Chapter 7. I have removed the background grid from this figure because otherwise the figure was becoming too busy. Whenever possible, design your figures so they don’t need a legend. Let’s consider another example that also demonstrates a very common visualization mistake. The figure contains four lines, representing the stock prices of four different companies. The lines are color coded using a colorblind-friendly color scale. So it should be relatively straightfoward to associate each line with the corresponding company. Yet it is not. The problem here is that the data lines have a clear visual order. The yellow line, representing Facebook, is clearly the highest line, and the black line, representing Apple, is clearly the lowest, with Alphabet and Microsoft inbetween, in that order. Yet the order of the four companies in the legend is Alphabet, Apple, Facebook, Microsoft (alphabetic order). Thus, the perceived order of the data lines differs from the order of the companies in the legend, and it takes a surprising amount of effort to match data lines with company names. This problem arises commonly with plotting software that autogenerates legends, as is the case for instance with R’s ggplot2. The plotting software has no concept of the visual order that the viewer will perceive in the data, and it will instead sort the legend by some other order, most commonly alphabetical. We can fix this problem by manually reordering the entries in the legend so they match the preceived ordering in the data. While the above figure is a major improvement, we can still do better. Even with the correct ordering, a legend imposes an unnecessary mental burden on the viewer of the figure. The viewer has to match the colors in the legend to the colors in the figure and translate which company name goes with which data line. It is better to get rid of the legend altogether and instead draw the company names right next to the data lines. I’ll provide a few more examples. First, in a density plot we may want to label the density curves directly rather than adding a color legend. Second, we can also combine this concept with the scatter plot from the beginning of this chapter to avoid a legend in that plot. And finally, here is an example where the color bar (representing temperature) is integrated into the temperature axis. "],
["small-axis-labels.html", "4 Your axis labels are too small", " 4 Your axis labels are too small If you take away only one single lesson from this book, make it this one: Pay attention to your axis labels, axis tick labels, and other assorted plot annotations. Chances are they are too small. In my experience, nearly all plot libraries and graphing softwares have poor defaults. If you use the default values, you’re almost certainly making a poor choice. For example, consider Figure 4.1. I see figures like this all the time. The axis labels, axis tick labels, and legend labels are all incredibly small. We can barely see them, and we may have to zoom into the page to distinguish front-wheel drive (FWD) from rear-wheel drive (RWD) in the figure legend. Figure 4.1: Fuel economy versus engine displacement, for cars with front-wheel drive (FWD), rear-wheel drive (RWD), and all-wheel drive (4WD). This figure suffers from the common affliction that the text elements are way too small and are barely legible. A somewhat better version of this figure is shown as Figure 4.2. I think the fonts are still too small, and that’s why I have labeled the figure as ugly. However, we are moving in the right direction. This figure might be passable under some circumstances. My main criticism here is not so much that the labels aren’t legible as that the figure is not balanced; the text elements are too small compared to the rest of the figure. Figure 4.2: Fuel economy versus engine displacement. This figure is an improvement over Figure ??, but the text elements remain too small and the figure is not balanced. The next figure uses the default settings I’m applying throughout this book. I think it is well balanced, the text is clearly visible, and it fits with the overall size of the figure. Figure 4.3: Fuel economy versus engine displacement. All figure elements are appropriately scaled. Importantly, we can overdo it and make the labels too big (Figure 4.4). Sometimes we need big labels, in particular if the figure is meant to be reduced in size, but the various elements of the figure (in particular, label text and plot symbols) need to fit together. In Figure 4.4, the points used to visualize the data are too small relative to the text. Once we fix this issue, the figure becomes acceptable again (Figure 4.5). Figure 4.4: Fuel economy versus engine displacement. The text elements are fairly large, and their size may be appropriate if the figure is meant to be reproduced at a very small scale. However, the figure overall is not balanced; the points are too small relative to the text elements. Figure 4.5: Fuel economy versus engine displacement. All figure elements are sized such that the figure is balanced and can be reproduced at a small scale. You may look at Figure 4.5 and find everything too big. However, keep in mind that it is meant to be scaled down. Scale the figure down so that it is only an inch or two in width, and it looks just fine. In fact, at that scaling this is the only figure in this chapter that looks good. Always look at scaled-down versions of your figures to make sure the axis labels are appropriately sized. I think there is a simple psychological reason for why we routinely make figures whose axis labels are too small, and it relates to large, high-resolution computer monitors. We routinely preview figures on the computer screen, and often we do so while the figure takes up a large amount of space on the screen. In this viewing mode, even comparatively small text seems perfectly fine and legible, and large text can seem awkward and overpowering. In fact, if you take the first figure from this chapter and magnify it to the point where it fills your entire screen, you will likely think that it looks just fine. The solution is to always make sure that you look at your figures at a realistic print size. You can either zoom out so they are only three to five inches in width on your screen, or you can go to the other side of your room and check whether the figure still looks good from a substantial distance. "],
["background-grids.html", "5 Background grids", " 5 Background grids With the rising popularity of the R package ggplot2, which uses a gray background grid as default, graphs with this style have become widespread. With apologies to ggplot2 author Hadley Wickham, for whom I have the utmost respect, I don’t find this style particularly attractive. In general, I find that the gray background detracts from the actual data. As an example, consider Figure 5.1, which shows the stock price of four major tech companies, indexed to their value in June 2012. The grid is too busy, and the gray background in the legend is distracting. Figure 5.1: Stock price over time for four major tech companies. The stock price for each company has been normalized to equal 100 in June 2012. We could try to remove the grid altogether, but I think that is a worse option (Figure 5.2). Now the curves seem to just float in space, and it’s difficult to see where they go. In addition, since all prices are indexed to 100 in June 2012, at a minimum this value should be marked in the plot. Thus, one option would be to add a thin horizontal line at y = 100 (Figure 5.3). Figure 5.2: Indexed stock price over time for four major tech companies. Figure 5.3: Indexed stock price over time for four major tech companies. Alternatively, we can use just a minimal grid. In particular, for a plot where we are primarily interested in the change in y values, vertical grid lines are not needed. Moreover, grid lines positioned at only the major axis ticks will often be sufficient. And, the axis line can be omitted or made very thin (Figure 5.4). Figure 5.4: Indexed stock price over time for four major tech companies. For such a minimal grid, we generally draw the lines orthogonally to direction along which the numbers of interest vary. Therefore, if instead of plotting the stock price over time we plot the five-year increase, as horizontal bars, then we will want to use vertical lines instead (Figure 5.5). Figure 5.5: Percent increase in stock price from June 2012 to June 2017, for four major tech companies. Grid lines that run perpendicular to the key variable of interest tend to be the most useful. Background grids along both axis directions can make sense, however, for scatter plots where there is no primary axis of interest. This presentation frequently looks best without axis lines. Figure 5.6 provides an example. Figure 5.6: Sepal length vs sepal width for 50 samples each from three different species of irises. For figures where the relevant comparison is the x = y line, I prefer to draw a diagonal line rather than a grid. For example, consider Figure 5.7, which compares two sets of correlations for 209 protein structures. By drawing the diagonal line, we can see immediately which correlations are systematically stronger. The same observation is much harder to make when the figure has a background grid instead (Figure 5.8). Thus, even though Figure 5.8 looks pleasing, I label it as bad. Figure 5.7: Correlations between evolutionary conservation and structural features of sites in 209 proteins. Along the y axis, we plot the correlation between evolutonary conservation (measured as evolutionary rate) at individual sites in a protein and the relative solvent accessibility (RSA) of those sites in the protein structure. Along the x axis, we plot the correlation between rate and weighted contact number (WCN), a measure for the density of contacts of a site in the protein structure. Each point represents one protein. We see that evolutionary conservation and structural features are highly correlated in some proteins and not very much in others. We also see that WCN, on average, yields somewhat stronger correlations than RSA does. Adapted from Echave, Spielman, and Wilke (2016). ## List of 58 ## $ line :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.636 ## ..$ linetype : num 1 ## ..$ lineend : chr &quot;butt&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ rect :List of 5 ## ..$ fill : chr &quot;transparent&quot; ## ..$ colour : logi NA ## ..$ size : num 0 ## ..$ linetype : num 0 ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ text :List of 11 ## ..$ family : chr &quot;&quot; ## ..$ face : chr &quot;plain&quot; ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 14 ## ..$ hjust : num 0.5 ## ..$ vjust : num 0.5 ## ..$ angle : num 0 ## ..$ lineheight : num 0.9 ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 0 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 6 0 3 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 7 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num 90 ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 6 0 3 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.title.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 0 7 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 12 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 3 0 0 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.x.top :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : num 0 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 2.8 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 1 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 3 0 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.text.y.right :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 0 2.8 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ axis.ticks :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ axis.ticks.length :Class &#39;unit&#39; atomic [1:1] 3.5 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## $ axis.line :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : NULL ## ..$ lineend : chr &quot;square&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ axis.line.x :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : NULL ## ..$ lineend : chr &quot;square&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ axis.line.y :List of 6 ## ..$ colour : chr &quot;black&quot; ## ..$ size : num 0.5 ## ..$ linetype : NULL ## ..$ lineend : chr &quot;square&quot; ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ legend.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0.2 0.2 0.2 0.2 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 1 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## $ legend.spacing :Class &#39;unit&#39; atomic [1:1] 0.4 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 1 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## $ legend.spacing.x : NULL ## $ legend.spacing.y : NULL ## $ legend.key : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.key.size :Class &#39;unit&#39; atomic [1:1] 1 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 3 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;lines&quot; ## $ legend.key.height : NULL ## $ legend.key.width : NULL ## $ legend.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size :Class &#39;rel&#39; num 0.857 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.text.align : NULL ## $ legend.title :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : num 0 ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ legend.title.align : NULL ## $ legend.position : chr &quot;right&quot; ## $ legend.direction : NULL ## $ legend.justification : chr [1:2] &quot;left&quot; &quot;center&quot; ## $ legend.box : NULL ## $ legend.box.margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 0 0 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 1 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## $ legend.box.background: list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ legend.box.spacing :Class &#39;unit&#39; atomic [1:1] 0.4 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 1 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## $ panel.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.border : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ panel.spacing :Class &#39;unit&#39; atomic [1:1] 7 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## $ panel.spacing.x : NULL ## $ panel.spacing.y : NULL ## $ panel.grid :List of 6 ## ..$ colour : chr &quot;white&quot; ## ..$ size : NULL ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.grid.minor :List of 6 ## ..$ colour : chr &quot;grey98&quot; ## ..$ size : num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi FALSE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## $ panel.ontop : logi FALSE ## $ plot.background : list() ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; ## $ plot.title :List of 11 ## ..$ family : NULL ## ..$ face : chr &quot;bold&quot; ## ..$ colour : NULL ## ..$ size : num 14 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 7 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.subtitle :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size :Class &#39;rel&#39; num 0.9 ## ..$ hjust : num 0 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 0 0 6.3 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.caption :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size :Class &#39;rel&#39; num 0.9 ## ..$ hjust : num 1 ## ..$ vjust : num 1 ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 6.3 0 0 0 ## .. .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ plot.margin :Classes &#39;margin&#39;, &#39;unit&#39; atomic [1:4] 7 7 7 7 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 8 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;pt&quot; ## $ strip.background :List of 5 ## ..$ fill : chr &quot;grey80&quot; ## ..$ colour : chr &quot;grey50&quot; ## ..$ size : num 0 ## ..$ linetype : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_rect&quot; &quot;element&quot; ## $ strip.placement : chr &quot;inside&quot; ## $ strip.text :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size :Class &#39;rel&#39; num 0.857 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.text.x : NULL ## $ strip.text.y :List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : NULL ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : num -90 ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi TRUE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## $ strip.switch.pad.grid:Class &#39;unit&#39; atomic [1:1] 0.1 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 1 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## $ strip.switch.pad.wrap:Class &#39;unit&#39; atomic [1:1] 0.1 ## .. ..- attr(*, &quot;valid.unit&quot;)= int 1 ## .. ..- attr(*, &quot;unit&quot;)= chr &quot;cm&quot; ## $ panel.grid.major :List of 6 ## ..$ colour : chr &quot;grey90&quot; ## ..$ size : num 0.5 ## ..$ linetype : NULL ## ..$ lineend : NULL ## ..$ arrow : logi FALSE ## ..$ inherit.blank: logi FALSE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_line&quot; &quot;element&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi TRUE ## - attr(*, &quot;validate&quot;)= logi TRUE Figure 5.8: Correlations between evolutionary conservation and structural features of sites in 209 proteins. By plotting this dataset against a background grid, the systematic shift of all points away from the x = y line is obscured. In summary, there is no simple choice of background grid that always works. I encourage you to think carefully about which specific grid or guidelines are most informative for the plot you are making, and to only show those. In general, less is more. Too many or overly thick and dark grid lines can distract your reader’s attention away from the data you want to show. References "],
["avoid-line-drawings.html", "6 Avoid line drawings", " 6 Avoid line drawings Whenever possible, visualize your data with solid, colored shapes rather than with lines that outline those shapes. Solid shapes are more easily perceived, are less likely to create visual artifacts or optical illusions, and do more immediately convey amounts than do outlines. In my experience, visualizations using solid shapes are both clearer and more pleasant to look at than equivalent versions that use line drawings. Thus, I avoid line drawings as much as possible. However, I want to emphasize that this recommendation does not supersede the principle of proportional ink (Chapter ??). Line drawings have a long history in the field of data visualization because throughout most of the 20th century, scientific visualizations were drawn by hand and had to be reproducible in black-and-white. This precluded the use of areas filled with solid colors, including solid gray-scale fills. Instead, filled areas were sometimes simulated by applying hatch, cross-hatch, or stipple patterns. Early plotting software imitated the hand-drawn simulations and similarly made extensive use of line drawings, dashed or dotted line patterns, and hatching. While modern visualization tools and modern reproduction and publishing platforms have none of the earlier limitations, many plotting softwares still default to outlines and empty shapes rather than filled areas. To raise your awareness of this issue, here I’ll show you several examples of the same figures drawn with both lines and filled shapes. The most common and at the same time most inappropriate use of line drawings is seen in histograms and bar plots. The problem with bars drawn as outlines is that it is not immediately apparent which side of any given line is inside a bar and which side is outside. As a consequence, in particular when there are gaps between bars, we end up with a confusing visual pattern that detracts from the main message of the figure (Figure 6.1). Filling the bars with a light color, or with gray if color reproduction is not possible, avoids this problem (Figure 6.2). Figure 6.1: Histogram of the ages of Titanic passengers, drawn with empty bars. The empty bars create a confusing visual pattern. In the center of the histogram, it is difficult to tell which parts are inside of bars and which parts are outside. Figure 6.2: The same histogram of Figure 6.1, now drawn with filled bars. The shape of the age distribution is much more easily discernible in this variation of the figure. Next, let’s take a look at an old-school density plot. I’m showing density estimates for the sepal-length distributions of three species of iris, drawn entirely in black-and-white as a line drawing (Figure 6.3). The distributions are shown just by their outlines, and because the figure is in black-and-white, we’re using different line styles to distinguish them. This figure has two main problems. First, the dashed line styles do not provide a clear separation between the area under the curve and the area above it. While our visual system is quite good at connecting the individual line elements into a continuous line, the dashed lines nevertheless look porous and do not serve as a strong boundary of the enclosed area. Second, because the lines intersect and the areas they enclose are not shaded, we can perceive six different distinct shapes and we need to expend some additional mental energy to merge the correct shapes together. This effect would have been even stronger had I used solid rather than dashed lines for all three distributions. Figure 6.3: Density estimates of the sepal lengths of three different iris species. The broken line styles used for versicolor and virginica detract from the perception that the areas under the curves are distinct from the areas above them. We can attempt to address the problem of porous boundaries by using colored lines rather than dashed lines (Figure 6.4). However, the density areas in the resulting plot still have little visual presence. Overall, I find the version with filled areas (Figure 6.5) the most clear and intuitive. It is important, however, to make the filled areas partially transparent, so that the complete distribution for each species is visible. Figure 6.4: Density estimates of the sepal lengths of three different iris species. By using solid, colored lines we have solved the probme of Figure 6.3 that the areas below and above the lines seem to be connected. However, we still don’t have a strong sense of the size of the area under each curve. Also, the colored labels are hard to read. Figure 6.5: Density estimates of the sepal lengths of three different iris species, shown as partially transparent shaded areas. Line drawings also arise in the context of scatter plots, when different point types are drawn as open circles, triangles, crosses, etc. As an example, consider Figure 6.6. The figure contains a lot of visual noise, and the different point types do not strongly separate from each other. Drawing the same figure with solidly colored shapes addresses this issue (Figure 6.7). Figure 6.6: Fuel economy versus engine displacement, for cars with front-wheel drive (FWD), rear-wheel drive (RWD), and all-wheel drive (4WD). The different point styles, all black-and-white line-drawn symbols, create substantial visual noise and make it difficult to read the figure. Figure 6.7: Fuel economy versus engine displacement. By using both different colors and different solid shapes for the different drive-train variants, this figure clearly separates the drive-train variants while remaining reproducible in gray scale if needed. I strongly prefer solid points over open points, because the solid points have much more visual presence. The argument that I sometimes hear in favor of open points is that they help with overplotting, since the empty areas in the middle of each point allow us to see other points that may be lying underneath. In my opinion, the benefit from being able to see overplotted points does not, in general, outweight the detriment from the added visual noise of open symbols. There are other approaches for dealing with overplotting, see Chapter 7 for some suggestions. I want to conclude this chapter with a brief discussion of boxplots. Boxplots are commonly drawn with empty boxes, as in Figure 6.8. I prefer a light shading for the box, as in Figure 6.9. The shading separates the box more clearly from the plot background, and it helps in particular when we’re showing many boxplots right next to each other, as is the case in Figures 6.8 and 6.9. In Figure 6.8, the large number of boxes and lines can again create the illusion of background areas outside of boxes being actually on the inside of some other shape, just as we saw in Figure 6.1. This problem is eliminated in Figure 6.9. I have sometimes heard the critique that shading the inside of the box gives too much weight to the center 50% of the data, but I don’t buy that argument. It is inherent to the boxplot, shaded box or not, to give more weight to the center 50% of the data than to the rest. If you don’t want this emphasis, then don’t use a boxplot. Instead, use a violin plot, jittered points, or a sina plot (Chapter 12). Figure 6.8: Distributions of daily mean temperatures in Lincoln, Nebraska, in 2016. Boxes are drawn in the draditional way, without shading. Figure 6.9: Distributions of daily mean temperatures in Lincoln, Nebraska, in 2016. By giving the boxes a light gray shading, we can make them stand out better against the background. "],
["overlapping-points.html", "7 Handling overlapping points 7.1 Partial transparency 7.2 Jittering 7.3 Contour lines 7.4 2d histograms", " 7 Handling overlapping points When we want to visualize large or very large datasets, we often experience the challenge that simple x–y scatter plots do not work very well because many points lie on top of each other and partially or fully overlap. And similar problems can arise even in small datasets if values were recorded with low precision or rounded, such that multiple observations have exactly the same numeric values. The technical term commonly used to describe this situation is “overplotting”, i.e., plotting many points on top of each other. Below, I describe several strategies you can pursue when you encounter this challenge. 7.1 Partial transparency Let’s revisit the fuel economy figure from Chapter 4 and focus on an aspect we ignored there: I have labeled this figure “bad” here because the points overlap and partly obscure each other. A simple way to ameliorate this issue is to use partial transparency: 7.2 Jittering Making points partially transparent is not always sufficient. For example, the same dataset contains fuel economy for both city and highway driving. If we plot those two quantities against each other, we obtain the following figure. Because fuel economy is rounded to whole integers in this dataset, many points lie exactly on top of each other. While these fully overlapping points appear darker in the plot, the visual appearance is that of one darker point rather than of a set of points plotted in the same location. We can emphasize the number of points in the same locations by applying a small amount of jitter, i.e., displacing each point randomly by a small amount. However, when jittering we have to make sure not to overdo it. If we jitter too much, we end up placing points in locations that are not representative of the underlying dataset and hence are creating a misleading visualization of the data. For example, in this particular case the extreme jittering creates the impression that for some cars the highway economy can fall below the city economy. However, such cases do not exist in the original dataset. 7.3 Contour lines When the number of points grows large, it can be helpful to indicate the point density, for example with contour lines. This technique works well for the following figure, which shows the total population as a function of area for counties in the midwest. If we want to emphasize the overall features of the distribution rather than the individual points, we can also show only the contour lines and leave out the individual points. Finally, we can add a smoothing line to highlight the overall trend in the relationship between the two variables. 7.4 2d histograms None of the techniques discussed so far work very well when the majority of points falls into a small area relative to the overall extent of the data and the overall extent highlights important data features. Consider the following figure, which shows the departure delay in minutes versus the flight departure time, for all flights departing Newark airport (EWR) in 2013. ## Warning in .parse_hms(..., order = &quot;HM&quot;, quiet = quiet): Some strings ## failed to parse, or all strings are NAs Even though we have made the points fairly transparent, the majority of the points just form a black band between 0 and 300 minutes departure delay. This black band obscures whether most flights depart approximately on time or with substantial delay (say 50 minutes or more). At the same time, the most delayed flights (with delays of 400 minutes or more) are barely visible due to the transparency of the points. A good solution for this particular case is a 2d histogram, where we subdivide the entire x–y plane into small squares, count how many observations fall into each square, and then color the square by that count. The result is the following figure. This figure clearly highlights several important features. First, the vast majority of departures during the day (6am to about 9pm) actually depart without delay or even early (negative delay). However, a modest number of departures has a substantial delay. Moreover, the later a plane departs in the day the more of a delay it can have. Importantly, here the departure time is the actual time of departure, not the scheduled time of departure. So this figure does not necessarily tell us that planes scheduled to depart early never experience delay. What it does tell us, though, is that if a plane departs early it either has little delay or, in very rare cases, a delay of around 900 minutes. As an alternative to binning the data into squares, we can also bin into hexagons. This approach, first proposed by Carr et al. (1987), has the advantage that the points in a hexagon are, on average, closer to the hexagon center than the points in an equal-area square are to the center of the square. As a consequence, the colored hexagon represents the data slightly more accurately than the colored square does. The following figure shows the same data with hexagon binning rather than square binning. References "],
["directory-of-visualizations.html", "8 Directory of visualizations 8.1 Proportions 8.2 Individual distributions 8.3 Multiple distributions 8.4 Other", " 8 Directory of visualizations 8.1 Proportions Proportions can be visualized as pie charts, side-by-side bars, or stacked bars, the latter arranged either vertically or horizontally (Chapter 9). Pie charts emphasize that the individual parts add up to a whole and highlight simple fractions. However, the individual pieces are more easily compared in side-by-side bars. Stacked bars look awkward for a single set of proportions, but can be useful when comparing multiple sets of proportions (see below). When visualizing multiple sets of proportions or changes in proportions across conditions, pie charts tend to be space-inefficient and often obscure relationships. Side-by-side bars work well as long as the number of conditions compared is moderate, and stacked bars can work for large numbers of conditions. Stacked densities (Chapter 9) are appropriate when the proportions change along a continuous variable. 8.2 Individual distributions Histograms and density plots (Chapter 10) provide the most intuitive visualizations of a distribution, but both require arbitrary parameter choices and can be misleading. Cumulative densities and q-q plots (Chapter 11) always represent the data faithfully but can be more difficult to interpret. 8.3 Multiple distributions Boxplots, violin plots, jittered points, and sina plots are useful when we want to visualize many distributions at once and/or if we are primarily interested in overall shifts among the distributions (Chapter 12). Stacked histograms and overlapping densities allow a more in-depth comparison of a smaller number of distributions, though stacked histograms can be difficult to interpret and are best avoided (Chapter 10). Ridgeline plots can be a useful alternative to violin plots and are often useful when visualizing very large numbers of distributions or changes in distributions over time (Chapter 12). 8.4 Other "],
["visualizing-proportions.html", "9 Visualizing proportions 9.1 A case for pie charts 9.2 A case for side-by-side bars 9.3 A case for stacked bars and stacked densities 9.4 Visualizing proportions separately as parts of the total", " 9 Visualizing proportions We often want to show how some group, entity, or amount breaks down into individual pieces that each represent a proportion of the whole. Common examples include the proportions of men and women in a group of people, the percentages of people voting for different political parties in an election, or the market shares of companies. The archetypical such visualization is the pie chart, omnipresent in any business presentation and much maligned among data scientists. As we will see, visualizing proportions can be challenging, in particular when the whole is broken into many different pieces or when we want to see changes in proportions over time or across conditions. There is no single ideal visualization that always works. To illustrate this issue, I discuss a few different scenarios that each call for a different type of visualization. Remember: You always need to pick the visualization that best fits your specific dataset and that highlights the key data features you want to show. 9.1 A case for pie charts From 1961 to 1983, the German parliament (called the Bundestag) was composed of members of three different parties, CDU/CSU, SPD, and FDP. During most of this time, CDU/CSU and SPD had approximately comparable numbers of seats, while the FDP typically held only a small fraction of seats. For example, in the 8th Bundestag, from 1976–1980, the CDU/CSU held 243 seats, SPD 214, and FDP 39, for a total of 496. Such parliamentary data is most commonly visualized as a pie chart (Figure 9.1). Figure 9.1: Party composition of the 8th German Bundestag, 1976–1980, visualized as a pie chart. This visualization shows clearly that the ruling coalition of SPD and FDP had a small majority over the opposition CDU/CSU. A pie chart breaks a circle into slices such that the area of each slice is proportional to the fraction of the total it represents. The same procedure can be performed on a rectangle, and the result is a stacked bar chart (Figure 9.2). Depending on whether we slice the bar vertically or horizontally, we obtain vertically stacked bars (Figure 9.2a) or horizontally stacked bars (Figure 9.2b). Figure 9.2: Party composition of the 8th German Bundestag, 1976–1980, visualized as stacked bars. (a) Bars stacked vertically. (b) Bars stacked horizontally. It is not immediately obvious that SPD and FDP jointly had more seats than CDU/CSU. We can also take the bars from Figure 9.2a and place them side-by-side rather than stacking them on top of each other. This visualization makes it easier to perform a direct comparison of the three groups, though it obscures other aspects of the data (Figure 9.3). Most importantly, in a side-by-side bar plot the relationship of each bar to the total is not visually obvious. Figure 9.3: Party composition of the 8th German Bundestag, 1976–1980, visualized as side-by-side bars. As in Figure 9.2, it is not immediately obvious that SPD and FDP jointly had more seats than CDU/CSU. Many authors categorically reject pie charts and argue in favor of side-by-side or stacked bars. Others defend the use of pie charts in some applications. My own opinion is that none of these visualizations is consistently superior over any other. Depending on the features of the dataset and the specific story you want to tell, you may want to favor one or the other approach. In the case of the 8th German Bundestag, I think that a pie chart is the best option. It shows clearly that the ruling coalition of SPD and FDP jointly had a small majority over the CDU/CSU (Figure 9.1). This fact is not visually obvious in any of the other plots (Figures 9.2 and 9.3). In general, pie charts work well when the goal is to emphasize simple fractions, such as one-half, one-third, or one-quarter. They also work well when we have very small datasets. A single pie chart, as in Figure 9.1, looks just fine, but a single column of stacked bars, as in Figure 9.2a, looks awkward. Stacked bars, on the other hand, can work for side-by-side comparisons of multiple conditions or in a time series, and side-by-side bars are preferred when we want to directly compare the individual fractions to each other. A summary of the various pros and cons of pie charts, stacked bars, and side-by-side bars is provided in Table 9.1. Table 9.1: Pros and cons of common apporaches to visualizing proportions: pie charts, stacked bars, and side-by-side bars. Pie chart Stacked bars Side-by-side bars Clearly visualizes the data as proportions of a whole ✔ ✔ ✖ Allows easy visual comparison of the relative proportions ✖ ✖ ✔ Visually emphasizes simple fractions, such as 1/2, 1/3, 1/4 ✔ ✖ ✖ Looks visually appealing even for very small datasets ✔ ✖ ✔ Works well when the whole is broken into many pieces ✖ ✖ ✔ Works well for the visualization of many sets of proportions or time series of proportions ✖ ✔ ✖ 9.2 A case for side-by-side bars I will now demonstrate a case where pie charts fail. This example is modeled after a critiqute of pie charts originally posted on Wikipedia (Wikipedia 2007). Consider the hypothetical scenario of five companies, A, B, C, D, and E, who all have roughly comparable market share of approximately 20%. Our hypothetical dataset lists the marketshare of each company for three consecutive years. When we visualize this dataset with pie charts, it is difficult to see what exactly is going on (Figure 9.4). It appears that the market share of company A is growing and the one of company E is shrinking, but beyond this one observation we can’t tell what’s going on. In particular, it is unclear how exactly the market shares of the different companies compare within each year. Figure 9.4: Market share of five hypothetical companies, A–E, for the years 2015–2017, visualized as pie charts. This visualization has two major problems: 1. A comparison of relative market share within years is nearly impossible. 2. Changes in market share across years are difficult to see. The picture becomes a little clearer when we switch to stacked bars (Figure 9.5). Now the trends of a growing market share for company A and a shrinking market share for company E are clearly visible. However, the relative market shares of the five companies within each year are still hard to compare. And it is difficult to compare the market shares of companies B, C, and D across years, because the bars are shifted relative to each other across years. This is a general problem of stacked-bar plots, and the main reason why I normally not recommend this type of visualization. Figure 9.5: Market share of five hypothetical companies for the years 2015–2017, visualized as stacked bars. This visualization has two major problems: 1. A comparison of relative market shares within years is difficult. 2. Changes in market share across years are difficult to see for the middle companies B, C, and D, because the location of the bars changes across years. For this hypothetical data set, side-by-side bars are the best choice (Figure 9.6). This visualization highlights that both companies A and B have increased their market share from 2015 to 2017 while both companies D and E have reduced theirs. It also shows that market shares increase sequentially from company A to E in 2015 and similarly decrease in 2017. Figure 9.6: Market share of five hypothetical companies for the years 2015–2017, visualized as side-by-side bars. 9.3 A case for stacked bars and stacked densities In Section 9.2, I wrote that I don’t normally recommend sequences of stacked bars, because the location of the internal bars shifts along the sequence. However, the problem of shifting internal bars disappears if there are only two bars in each stack, and in those cases the resulting visualization can be exceptionally clear. As an example, consider the proportion of women in a country’s national parliament. We will specifically look at the African country Rwanda, which as of 2016 tops the list of countries with the highest proportion of female parliament memebers. Rwanda has had a majority female parliament since 2008, and since 2013 nearly two-thirds of its members of parliament are female. To visualize how the proportion of women in the Rwandan parliament has changed over time, we can draw a sequence of stacked bar graphs (Figure 9.7). This figure provides an immediate visual representation of the changing proportions over time. To help the reader see exactly when the majority turned female, I have added a thin horizontal line at 50%. Without this line, it would be near impossible to determine whether from 2003 to 2007 the majority was male or female. I have not added similar lines at 25% and 75%, to avoid making the figure too cluttered. (See Chapter 5 for further discussion on such lines.) Figure 9.7: Change in the gender composition of the Rwandan parliament over time, 1997 to 2016. Source: Inter-Parliamentary Union (IPU), ipu.org. If we want to visualize how proportions change in response to a continuous variable, we can switch from stacked bars to stacked densities. Stacked densities can be thought of as the limiting case of infinitely many infinitely small stacked bars arranged side-by-side. The densities in stacked-density plots are typically obtained from kernel density estimation, as described in Chapter 10, and I refer you to that chapter for a general discussion of the strengths and weaknesses of this method. To give an example where stacked densities may be appropriate, consider the health status of people as a function of age. Age can be considered a continuous variable, and visualizing the data in this way works reasonably well (Figure 9.8). Even though we have four health categories here, and I’m generally not a fan of stacking multiple conditions, as discussed above, I think in this case the figure is acceptable. We can see clearly that overall health declines as people age, and we can also see that despite this trend, over half of the population remain in good or excellent health until very old age. Figure 9.8: Health status by age, as reported by the general social survey (GSS). Nevertheless, this figure has a major limitation: By visualizing the porportions of the four health conditions as percent of the total, the figure obscures that there are many more young people than old people in the dataset. Thus, even though the percentage of people reporting to be in good health remains approximately unchanged across ages spanning seven decades, the absolute number of people in good health declines as the total number of people at a given age declines. I will present a potential solution to this problem in the next section. 9.4 Visualizing proportions separately as parts of the total Side-by-side bars have the problem that they don’t clearly visualize the size of the individual parts relative to the whole and stacked bars have the problem that the different bars cannot be compared easily because they have different baselines. We can resolve these two issues by making a separate plot for each part and in each plot showing the respective part relative to the whole. For the health dataset of Figure 9.8, this procedure results in Figure 9.9. The overall age distribution in the dataset is shown as the shaded gray areas, and the age distributions for each health status are shown in blue. This figure highlights that in absolute terms, the number people with excellent or good health declines past ages 30–40, while the number of people with fair health remains approximately constant across all ages. Figure 9.9: Health status by age, shown as proportion of the total number of people in the survey. The colored areas show the density estimates of the ages people with different health status, respectively, and the gray areas show the overall age distribution. To provide a second example, let’s consider a different variable from the same survey: marital status. Marital status changes much more drastically with age than does health status, and a stacked densities plot of marital status vs age is not very illuminating (Figure 9.10). Figure 9.10: Marital status by age, as reported by the general social survey (GSS). To simplify the figure, I have removed a small number of cases that report as separated. I have labeled this figure as bad because the frequency of people who have never been married or are widowed changes so drastically with age that the age distributions of maried and divorced people are highly distorted and difficult to interpret. The same dataset visualized as partial densities is much clearer (Figure 9.11). In particular, we see that the proportion of married people peaks around the late 30s, the proportion of divorced people peaks around the early 40s, and the proportion of widowed people peaks around the mid 70s. Figure 9.11: Marital status by age, shown as proportion of the total number of people in the survey. The colored areas show the density estimates of the ages people with different marital status, respectively, and the gray areas show the overall age distribution. References "],
["histograms-density-plots.html", "10 Visualizing distributions: Histograms and density plots 10.1 Visualizing a single distribution 10.2 Visualizing multiple distributions at the same time", " 10 Visualizing distributions: Histograms and density plots We frequently encounter the situation where we would like to understand how a particular variable is distributed in a dataset. To give a concrete example, consider the passengers of the ship Titanic, which sank on April 15, 2012. There were approximately 1317 passengers on the Titanic (not counting crew), and we have reported ages for 756 of them. We might want to know how many passengers of what ages there were on the Titanic, i.e., how many children, young adults, middle-aged people, seniors, and so on. We call the relative proportions of different ages among the passengers the age distribution of the passengers. 10.1 Visualizing a single distribution We can obtain a sense of the age distribution among the passengers by grouping all passengers into bins with comparable ages and then counting the number of passengers in each bin. This procedure results in a table such as Table 10.1. Table 10.1: Numbers of passenger with known age on the Titanic. age range count 0–5 36 6–10 19 11–15 18 16–20 99 21–25 139 26–30 121 age range count 31–35 76 36–40 74 41–45 54 46–50 50 51–55 26 56–60 22 age range count 61–65 16 66–70 3 71–75 3 We can visualize this table by drawing filled rectangles whose heights correspond to the counts and whose widths correspond to the width of the age bins (Figure 10.1). Such a visualization is called a histogram. Figure 10.1: Histogram of the ages of Titanic passengers. Because histograms are generated by binning the data, their exact visual appearance depends on the choice of the bin width. Most visualization programs that generate histograms will choose a bin width by default, but chances are that bin width is not the most appropriate one for any histogram you may want to make. It is therefore critical to always try different bin widths to verify that the resulting histogram reflects the underlying data accurately. In general, if the bin width is too small, then the histogram becomes overly peaky and visually busy and the main trends in the data may be obscured. On the other hand, if the bin width is too large, then smaller features in the distribution of the data may disappear. For the age distribution of Titanic passengers, we can see that a bin width of one year is too small and a bin width of fifteen years is too large, whereas bin widths between three to five years work fine (Figure 10.2). Figure 10.2: Histograms depend on the chosen bin width. Here, the same age distribution of Titanic passengers is shown with four different bin widths: (a) one year; (b) three years; (c) five years; (d) fifteen years. When making a histogram, always explore multiple bin widths. Histograms have been a popular visualization option since at least the 18th century, in part because they are easily generated by hand. More recently, as extensive computing power has become available in everyday devices such as laptops and cell phones, we see them increasingly being replaced by density plots. In a density plot, we attempt to visualize the underlying probability distribution of the data by drawing an appropriate continuous curve (Figure 10.3). This curve needs to be estimated from the data, and the most commonly used method for this estimation procedure is called kernel density estimation. In kernel density estimation, we draw a continuous curve (the kernel) with a small width (controlled by a parameter called bandwidth) at the location of each data point, and then we add up all these curves to obtain the final density estimate. The most widely used kernel is a Gaussian kernel (i.e., a Gaussian bell curve), but there are many other choices. Figure 10.3: Kernel density estimate of the age distribution of passengers on the Titanic. The height of the curve is scaled such that the area under the curve equals one. The density estimate was performed with a Gaussian kernel and a bandwidth of 2. Just as is the case with histograms, the exact visual appearance of a density plot depends on the kernel and bandwidth choices (Figure 10.4). The bandwidth parameter behaves similarly to the bin width in histograms. If the bandwidth is too small, then the density estimate can become overly peaky and visually busy and the main trends in the data may be obscured. On the other hand, if the bandwidth is too large, then smaller features in the distribution of the data may disappear. In addition, the choice of the kernel affects the shape of the density curve. For example, a Gaussian kernel will have a tendency to produce density estimates that look Gaussian-like, with smooth features and tails. By contrast, a rectangular kernel can generate the appearance of steps in the density curve (Figure 10.4d). In general, the more data points there are in the data set, the less the choice of the kernel matters. Therefore, density plots tend to be quite reliable and informative for large data sets but can be misleading for data sets of only a few points. Figure 10.4: Kernel density estimates depend on the chosen kernel and bandwidth. Here, the same age distribution of Titanic passengers is shown for four different combinations of these parameters: (a) Gaussian kernel, bandwidth = 0.5; (b) Gaussian kernel, bandwidth = 2; (c) Gaussian kernel, bandwidth = 5; (d) Rectangular kernel, bandwidth = 2. Density curves are usually scaled such that the area under the curve equals one. This convention can make the y axis scale confusing, because it depends on the units of the x axis. For example, in the case of the age distribution, the data range on the x axis goes from 0 to approximately 75. Therefore, we expect the mean height of the density curve to be 1/75 = 0.013. Indeed, when looking at the age density curves (e.g., Figure 10.4), we see that the y values range from 0 to approximately 0.04, with an average of somewhere close to 0.01. Kernel density estimates have one pitfall that we need to be aware of: They have a tendency to produce the appearance of data where none exists, in particular in the tails. As a consequence, careless use of density estimates can easily lead to figures that make nonsensical statements. For example, if we don’t pay attention, we might generate a visualization of an age distribution that includes negative ages (Figure 10.5). Figure 10.5: Kernel density estimates can extend the tails of the distribution into areas where no data exist and no data are even possible. Here, the density estimate has been allowed to extend into the negative age range. This is clearly nonsensical and should be avoided. Always verify that your density estimate does not predict the existence of nonsensical data values. So should you use a histogram or a density plot to visualize a distribution? Heated discussions can be had on this topic. Some people are vehemently against density plots and believe that they are arbitrary and misleading. Others realize that histograms can be just as arbitrary and misleading. I think the choice is largely a matter of taste, but sometimes one or the other option may more accurately reflect the specific features of interest in the data at hand. There is also the possibility of using neither and instead choosing empirical cumulative density functions or q-q plots (Chapter 11). Finally, I believe that density estimates have an inherent advantage over histograms as soon as we want to visualize more than one distribution at a time (see next section). 10.2 Visualizing multiple distributions at the same time In many scenarios we have multiple distributions we would like to visualize simultaneously. For example, let’s say we’d like to see how the ages of Titanic passengers are distributed between men and women. Were men and women passengers generally of the same age, or was there an age difference between the genders? One commonly employed visualization strategy in this case is a stacked histogram, where we draw the histogram bars for women on top of the bars for men, in a different color (Figure 10.6). Figure 10.6: Histogram of the ages of Titanic passengers stratified by gender. In my opinion, this type of visualization should be avoided. There are two key problems here: First, from just looking at the figure, it is never entirely clear where exactly the bars begin. Do they start where the color changes or are they meant to start at zero? In other words, are there about 25 females of age 18–20 or are there almost 80? (The former is the case.) Second, the bar heights for the female counts cannot be directly compared to each other, because the bars all start at a different height. For example, the men were on average older than the women, and this fact is not at all visible in Figure 10.6. We could try to address these problems by having all bars start at zero and making the bars partially transparent (Figure 10.7). Figure 10.7: Age distributions of male and female Titanic passengers, shown as two overlapping histograms. However, this approach generates new problems. Now it appears that there are actually three different groups, not just two, and we’re still not entirely sure where each bar starts and ends. Overlapping histograms don’t work well because a semi-transparent bar drawn on top of another tends to not look like a semi-transparent bar but instead like a bar drawn in a different color. Overlapping density plots don’t typically have the problem that overlapping histograms have, because the continuous density lines help the eye keep the distributions separate. However, for this particular dataset, the age distributions for male and female passengers are nearly identical up to around age 17 and then diverge, so that the resulting visualization is still not ideal (Figure 10.8). Figure 10.8: Density estimates of the ages of male and female Titanic passengers. To highlight that there were more male than female passengers, the density curves were scaled such that the area under each curve corresponds to the total number of male and female passengers with known age (468 and 288, respectively). A solution that works well for this dataset is to show the age distributions of male and female passengers separately, each as a proportion of the overall age distribution (Figure 10.9). This visualization shows intuitively and clearly that there were many fewer women than men in the 20–50-year age range on the Titanic. Figure 10.9: Age distributions of male and female Titanic passengers, shown as proportion of the passenger total. The colored areas show the density estimates of the ages of male and female passengers, respectively, and the gray areas show the overall passenger age distribution. Finally, when we want to visualize exactly two distributions, we can also make two separate histograms, rotate them by 90 degrees, and have the bars in one histogram point into the opposite direction of the other. This trick is commonly employed when visualizing age distributions, and the resulting plot is usually called an age pyramid (Figure 10.10). Figure 10.10: The age distributions of male and female Titanic passengers visualized as an age pyramid. Importantly, this trick does not work when there are more than two distributions we want to visualize at the same time. For example, to visualize the length distributions of sepals for three different iris species, density plots are by far the best choice (Figure 10.11). Figure 10.11: Density estimates of the sepal lengths of three different iris species. To visualize several distributions at once, kernel density plots will generally work better than histograms. "],
["ecdf-qq.html", "11 Visualizing distributions: Empirical cumulative density functions and q-q plots 11.1 Empirical cumulative density functions 11.2 Highly skewed distributions 11.3 Quantile–quantile plots", " 11 Visualizing distributions: Empirical cumulative density functions and q-q plots In Chapter 10, I described how we can visualize distributions with histograms or density plots. Both of these approaches are highly intuitive and visually appealing. However, as discussed in that chapter, they both share the limitation that the resulting figure depends to a substantial degree on parameters the user has to choose, such as the bin width for histograms and the bandwidth for density plots. As a result, both have to be considered as an interpretation of the data rather than a direct visualization of the data itself. As an alternative to using histograms or density plots, we could simply show all the data points individually, as a point cloud (see e.g. Chapter 12). However, this approach becomes unwieldy for very large datasets, and in any case there is value in aggregate methods that highlight properties of the distribution rather than the individual data points. To solve this problem, statisticians have invented empirical cumulative density functions (ecdfs) and quantile–quantile (q-q) plots. These types of visualizations require no arbitrary parameter choices, and they show all of the data at once. Unfortunately, they are a little less intuitive than a histogram or a density plot is, and I don’t see them used frequently outside of highly technical publications. They are quite popular among statisticians, though, and I think anybody interested in data visualization should be familiar with these techniques. 11.1 Empirical cumulative density functions To illustrate cumulative empirical density functions, I will begin with a hypothetical example that is closely modeled after something I deal with a lot as a professor in the classroom: a dataset of student grades. Assume our hypothetical class has 50 students, and the students just completed an exam on which they could score between 0 and 100 points. How can we best visualize the class performance, for example to determine appropriate grade boundaries? We can plot the total number of students that have received at least a certain number of points versus all possible point scores. This plot will be an ascending function, starting at 0 for 0 points and ending at 50 for 100 points. A different way of thinking about this visualization is the following: We can rank all students by the number of points they obtained, in ascending order (so the student with the fewest points receives the lowest rank and the student with the most points the highest), and then plot the rank versus the actual points obtained. The result is an empirical cumulative distribution function (ecdf) or simply cumulative distribution. Each dot represents one student, and the lines visualize the highest student rank observed for any possible point value (Figure 11.1). Figure 11.1: Empirical cumulative distribution function of student grades for a hypothetical class of 50 students. You may wonder what happens if we rank the students the other way round, in descending order. This ranking simply flips the function on its head. The result is still an empirical cumulative distribution function, but the lines now represent the lowest student rank observed for any possible point value (Figure 11.2). Figure 11.2: Distribution of student grades plotted as a descending ecdf. Ascending cumulative distribution functions are more widely known and more commonly used than descending ones, but both have important applications. Descending cumulative distribution functions are critical when we want to visualize highly skewed distributions (see Section 11.2). In practical applications, it is quite common to draw the ecdf without highlighting the individual points and to normalize the ranks by the maximum rank, so that the y axis represents the cumulative frequency (Figure 11.3). Figure 11.3: Ecdf of student grades. The student ranks have been normalized to the total number of students, such that the y values plotted correspond to the fraction of student in the class with at most that many points. We can directly read off key properties of the student grade distribution from this plot. For example, approximately a quarter of the students (25%) received less than 75 points. The median point value (corresponding to a cumulative frequency of 0.5) is 81. Approximately 20% of the students received 90 points or more. I find ecdfs handy for assigning grade boundaries because they help me locate the exact cutoffs that minimize student unhappiness. For example, in this example, there’s a fairly long horizontal line right below 80 points, followed by a steep rise right at 80. This feature is caused by three students receiving 80 points on their exam while the next poorer performing student received only 76. In this scenario, I might decide that everybody with a point score of 80 or more receives a B and everybody with 79 or less receives a C. The three students with 80 points are happy that they just made a B, and the student with 76 realizes that they would have had to perform much better to not receive a C. If I had set the cutoff at 77, the distribution of letter grades would have been exactly the same, but I might find the student with 76 points visiting my office hoping to negotiate their grade up. Likewise, if I had set the cutoff at 81, I would likely have had three students in my office trying to negotiate their grade. 11.2 Highly skewed distributions Many empirical datasets display highly skewed distributions, in particular with heavy tails to the right, and these distributions can be challenging to visualize. Examples of such distributions include the number of people living in different cities or counties, the number of contacts in a social network, the frequency with which individual words appear in a book, the number of academic papers written by different authors, the net worth of individuals, and the number of interaction partners of individual proteins in protein–protein interaction networks (Clauset, Shalizi, and Newman (2009)). All these distributions have in common that their right tail decays slower than an exponential function. In practice, this means that very large values are not that rare, even if the mean of the distribution is small. An important class of such distributions are power-law distributions, where the likelihood to observe a value that is x times larger than some reference point declines as a power of x. To give a concrete example, consider net worth in the US, which is distributed according to a power law with exponent 2. At any given level of net worth (say, $1 million), people with half that net worth are four times as frequent, and people with twice that net worth are one-fourth as frequent. Importantly, the same relationship holds if we use $10,000 as reference point or if we use $100 million. For this reason, power-law distributions are also called scale-free distributions. Here, I will first discuss the number of people living in different US counties according to the 2010 US Census. This distribution has a very long tail to the right. Even though most counties have relatively small numbers of inhabitants (the median is 25,857), a few counties have extremely large numbers of inhabitants (e.g., Los Angeles County, with 9,818,605 inhabitants). If we try to visualize the distribution of population counts as either a density plot or an ecdf, we obtain figures that are essentially useles (Figure 11.4). Figure 11.4: Distribution of the number of inhabitants in US counties, according to the 2010 US Census. (a) Density plot. (b) Empirical cumulative distribution function. The density plot (Figure 11.4a) shows a sharp peak right at 0 and virtually no details of the distribution are visible. Similarly, the ecdf (Figure 11.4b) shows a rapid rise near 0 and again no details of the distribution are visible. For this particular dataset, we can log-transform the data and visualize the distribution of the log-transformed values. This transformation works here because the population numbers in counties is not actually a power law, but instead follow a nearly perfect log-normal distribution (see Section 11.3). Indeed, the density plot of the log-transformed values shows a nice bell curve and the corresponding ecdf shows a nice sigmoidal shape (Figure 11.5). Figure 11.5: Distribution of the logarithm of the number of inhabitants in US counties. (a) Density plot. (b) Empirical cumulative distribution function. To see that this distribution is not a power law, we plot it as a descending ecdf with logarithmic x and y axes. In this visualization, a power law appears as a perfect straight line. For the population counts in counties, the right tail forms almost but not quite a straight line on the descending log-log ecdf plot (Figure 11.6). Figure 11.6: Relative frequency of counties with at least that many inhabitants versus the number of county inhabitants. As a second example, I will use the distribution of word frequencies for all words that appear in the novel Moby Dick. This distribution follows a perfect power law. When plotted as descending ecdf with logarithmic axes, we see a nearly perfect straight line (Figure 11.7). Figure 11.7: Distribution of word counts in the novel Moby Dick. Shown is the relative frequency of words that occur at least that many times in the novel versus the number of times words are used. 11.3 Quantile–quantile plots Quantile–quantile (q-q) plots are a useful visualization when we want to determine to what extent the observed data points do or do not follow a given distribution. Just like ecdfs, q-q plots are also based on ranking the data and visualizing the relationship between ranks and acutal values. However, in q-q plots we don’t plot the ranks directly, we use them to predict where a given data point should fall if the data were distributed according to a specified reference distribution. Most commonly, q-q plots are constructed using a normal distribution as the reference. To give a concrete example, assume the actual data values have a mean of 10 and a standard deviation of 3. Then, assuming a normal distribution, we would expect a data point ranked at the 50th percentile to lie at position 10 (the mean), a data point at the 84th percentile to lie at position 13 (one standard deviation above from the mean), and a data point at the 2.3rd percentile to lie at position 4 (two standard deviations below the mean). We can carry out this calculation for all points in the dataset and then plot the observed values (i.e., values in the dataset) against the theoretical values (i.e., values expected given each data point’s rank and the assumed reference distribution). When we perform this procedure for the student grades distribution from the beginning of this chapter, we obtain Figure 11.8. Figure 11.8: q-q plot of student grades. The solid line here is not a regression line but indicates the points where x equals y, i.e., where the observed values equal the theoretical ones. To the extent that points fall onto that line, the data follow the assumed distribution (here, normal). We see that the student grades follow mostly a normal distribution, with a few deviations at the bottom and at the top (a few students performed worse than expected on either end). The deviations from the distribution at the top end are caused by the maximum point value of 100 in the hypothetical exam; regardless of how good the best student is, he or she could at most obtain 100 points. We can also use a q-q plot to test my assertion from earlier in this chapter that the population counts in US counties follow a log-normal distribution. If these counts are log-normally distributed, then their log-transformed values are normally distributed and hence should fall right onto the x = y line. When making this plot, we see that the agreement between the observed and the theoretical values is exceptional (Figure 11.9). This demonstrates that the distribution of population counts among counties is indeed log-normal. Figure 11.9: q-q plot of the logarithm of the number of inhabitants in US counties. References "],
["boxplots-violins.html", "12 Boxplots, violin plots, and more", " 12 Boxplots, violin plots, and more We commonly have to visualize multiple distributions at the same time. For example, consider weather data. We have observations for each day in a month, possibly at multiple time points or multiple locations, but we frequently are interested in the broader trends, such as how temperature changes with month. The following figure visualizes temperature data collected in Lincoln, Nebraska in 2016. The dataset contains the mean temperature for each day of the year. We could plot this dataset by calculating the average mean temperature in each month and plotting it as points with error bars. However, there are multiple problems with this approach. First, we’re losing a lot of information about the data. Second, it’s not necessarily clear what the points represent. Third, it’s definitely not clear what the errorbars represent. There is no standard. Do they represent the standard deviation of the data, the standard error of the mean, a 95% confidence interval, or something else altogether? (I’m here plotting twice the standard deviation, to indicate the range that contains approximately 95% of the data.) Fourth, symmetric error bars are misleading if there is any skew in the data, which is the case here and almost always for real-world datasets. A traditional and commonly used method of visualizing key parameters of distributions is the boxplot. The boxplot divides the data into quartiles and visualizes them in a standardized manner. The line in the middle represents the median, and the box encloses the middle 50% of the data. The top and bottom wiskers extend either to the maximum and minimum of the data, respectively, or to 1.5 times the height of the box, whichever yields the shorter wisker. When the wiskers extend to 1.5 times the height of the box, they are called the upper and lower fence, respectively. Individual data points that fall beyond the upper or lower fence are referred to as outliers and usually showns as individual dots. When we visualize the temperature dataset using boxplots, we obtain the following result. Using the boxplot visualization, we see clearly that temperature is highly skewed in December (most days are moderately cold, and a few are extremely cold) and not very skewed at all in some other months, e.g., July. We can also plot all individual points: Finally, we can combine the best of both worlds and spread the dots out in proportion to the number of points with a similar y coordinate. This methods yields the sina plot, which shows each individual dot while also visualizing the distributions. "],
["understanding-the-most-commonly-used-image-file-formats.html", "13 Understanding the most commonly used image file formats 13.1 Bitmap and vector graphics 13.2 Lossless and lossy compression of bitmap graphics 13.3 Converting between image formats", " 13 Understanding the most commonly used image file formats Anybody who is making figures for data visualization will eventually have to know a few things about how figures are stored on the computer. There are many different image file formats, and each has its own set of benefits and disadvantages. Choosing the right file format and the right workflow can alleviate many figure-preparation headaches. My own preference is to use pdf for high-quality publication-ready files and generally whenever possible, png for online documents and other scenarios where bitmap graphics are required, and jpeg as the final resort if the png files are too large. In the following, I explain the key differences between these file formats and their respective benefits and drawbacks. 13.1 Bitmap and vector graphics The most important difference between the various graphics formats is whether they are bitmap or vector (Table 13.1). Bitmaps or raster graphics store the image as a grid of individual points (called pixels), each with a specified color. By contrast, vector graphics store the geometric arrangement of individual graphical elements in the image. Thus, a vector image contains information such as “there’s a black line from the top left corner to the bottom right corner, and a red line from the bottom left corner to the top right corner,” and the actual image is recreated on the fly as it is displayed on screen or printed. Table 13.1: Commonly used image file formats Acronym Name Type Application pdf Portable Document Format vector general purpose eps Encapsulated PostScript vector general purpose, outdated; use pdf svg Scalable Vector Graphics vector online use png Portable Network Graphics bitmap optimized for line drawings jpeg Joint Photographic Experts Group bitmap optimized for photographic images tiff Tagged Image File Format bitmap print production, accurate color reproduction raw Raw Image File bitmap digital photography, needs post-processing gif Graphics Interchange Format bitmap outdated, do not use Vector graphics are also called “resolution-independent,” because they can be magnified to arbitrary size without losing detail or sharpness. See Figure 13.1 for a demonstration. Figure 13.1: Illustration of the key difference between vector graphics and bitmaps. (a) Original image. The black square around the number seven indicates the area we’re magnifying in parts (b) and (c). (b) Increasing magnification of the highlighted area from part (a) when the image has been stored as a bitmap graphic. We can see how the image becomes increasingly pixelated and blurry as we zoom in further. (c) As part (b), but now for a vector representatin of the image. The image maintains perfect sharpness at arbitrary magnification levels. Vector graphics have two down-sides that can and often do cause trouble in real-world applications. First, because vector graphics are redrawn on the fly by the graphics program with which they are displayed, it can happen that there are differences in how the same graphic looks in two different programs, or on two different computers. This problem occurs most frequently with text, for example when the required font is not available and the rendering software substitutes a different font. Font substitutions will typically allow the viewer to read the text as intended, but the resulting image rarely looks good. There are ways to avoid these problems, such as outlining or embedding all fonts in a pdf file, but they may require special software and/or special technical knowledge to achieve. By contrast, bitmap images will always look correct. Second, for very large and/or complex figures, vector graphics can grow to enourmous file sizes and be slow to render. For example, a scatter plot of millions of data points will contain the x and y coordinates of every individual point, and each point needs to be drawn when the image is rendered, even if points overlap and/or are hidden by other graphical elements. As a consequence, the file may be many megabytes in size, and it may take the rendering software some time to display the figure. When I was a postdoc in the early 2000s, I once created a pdf file that at the time took almost an hour to display in the Acrobat reader. While modern computers are much faster and rendering times of many minutes are all but unheard of these days, even a rendering time of a few seconds can be disruptive if you want to embed your figure into a larger document and your pdf reader grinds to a halt every time you display the page with that one offending figure. Of course, on the flip side, simple figures with only a small number of elements (a few data points and some text, say) will often be much smaller as vector graphics than as bitmaps, and the viewing software may even render such figures faster than it would the corresponding bitmap images. 13.2 Lossless and lossy compression of bitmap graphics Most bitmap file formats employ some form of data compression to keep file sizes manageable. There are two fundamental types of compression: lossless and lossy. Lossless compression guarantees that the compressed image is pixel-for-pixel identical to the original image, whereas lossy compression accepts some image degradation in return for smaller file sizes. To understand which approach is appropriate when, it is helpful to have a basic understanding of how these different compression algorithms work. Let’s first consider lossless compression. Imagine an image with a black background, where large areas of the image are solid black and thus many black pixels appear right next to each other. Each black pixel can be represented by three zeroes in a row: 0 0 0, representing zero intensities in the red, green, and blue color channels of the image. The areas of black background in the image correspond to thousands of zeros in the image file. Now assume somewhere in the image are 1000 consecutive black pixels, corresponding to 3000 zeros. Instead of writing out all these zeros, we could store simply the total number of zeros we need, e.g. by writing 3000 0. In this way, we have conveyed the exact same information with only two numbers, the count (here, 3000) and the value (here, 0). Over the years, many clever tricks along these lines have been developed, and modern lossless image formats (such as png) can store bitmap data with impressive efficiency. However, all lossless compression algorithms perform best when images have large areas of uniform color, and therefore Table 13.1 lists png as optimized for line drawings. Photographic images rarely have multiple pixels of identical color and brightness right next to each other. Instead they have gradients and other somewhat regular patterns on many different scales. Therefore, lossless compression of these images often doesn’t work very well, and lossy compression has been developed as an alternative. The key idea of lossy compression is that some details in an image are too subtle for the human eye, and those can be discarded without obvious degradation in the image quality. For example, consider a gradient of 1000 pixels, each with a slightly different color value. Chances are the gradient will look nearly the same if it is drawn with only 200 different colors and coloring every five adjacent pixels in the exact same color. The most widely used lossy image format is jpeg (Table 13.1), and indeed many digital cameras output images as jpeg by default. Jpeg compression works exceptionally well for photographic images, and huge reductions in file-size can often be obtained with very little degradation in image quality. However, jpeg compression fails when images contain sharp edges, such as created by line drawings or by text. In those cases, jpeg compression can result in very noticeable artifacts (Figure 13.2). Figure 13.2: Illustration of jpeg artifacts. (a) The same image is reproduced multiple times using increasingly severe jpeg compression. The resulting file size is shown in the top-right corner of each image. A reduction in file size by a factor of 10, from 432kB in the original image to 43kB in the compressed image, results in only minor perceptible reduction in image quality. However, a further reduction in file size by a factor of 2, to a mere 25kB, leads to numerous visible artifacts. (b) Zooming in to the most highly compressed image reveals the various compression artifacts. Image credit: Claus O. Wilke 13.3 Converting between image formats It is generally possible to convert any image format into any other image format. For example, on a Mac, you can open an image with Preview and then export to a number of different formats. In this process, though, important information can get lost, and information is never regained. For example, after saving a vector graphic into a bitmap format, e.g. a pdf file as a jpeg, the resolution independence that is a key feature of the vector graphic has been lost. Conversely, saving a jpeg image into a pdf file does not magically turn the image into a vector graphic. The image will still be a bitmap image, just stored inside the pdf file. Similarly, converting a jpeg file into a png file does not remove any artifacts that may have been introduced by the jpeg compression algorithm. It is therefore a good rule of thumb to always store the original image in the format that maintains maximum resolution, accuracy, and flexibility. Thus, for data visualizations, create your figure as pdf and then convert into png or jpg when necessary. Similarly, for images that are only available as bitmaps, such as digital photographs, store them in a format that doesn’t use lossy compression, or if that can’t be done, compress as little as possible. Also, store the image in as high a resolution as possible, and down-scale when needed. "],
["notes.html", "14 Notes 14.1 Outline 14.2 Other notes and comments", " 14 Notes 14.1 Outline Chapters for which some material exists at this time are marked with stars, as follows: *Chapter started; made relevant figures and/or wrote rudimentary first draft **Chapter mostly complete Part I: General principles of figure design Visualizing data: mapping data onto aesthetics Explains the basic concept of aesthetic mapping, which lies at the heart of any data visualization. Figure titles Discusses when to use and not to use figure titles. For captioned figures, the titles are normally the first thing shown in the caption, and thus are not shown on top of the figure. Effective use of color in figures Covers basic concepts of color use, as a tool to highlight, as a tool to distinguish, and as a tool to represent a value. Introduces the three basic types of color scales: qualitative, directional, diverging. Choosing colorblind-friendly color scales* Explains what color choices can and cannot be seen by people with colorblindness. Redundant coding* Explains how to make sure that key information in the figure is provided in multiple, reduant ways, for example through color and location or color and direct labeling. Your axis labels are too small** Discusses the widespread problem of excessively small axis labels. Choosing the right axis settings Covers various aspects related to axis choice, including linear vs. logarithmic axes, as well as issues of axis expansion beyond the data range. Choosing an appropriate aspect ratio Discusses the concept of aspect ratio, including when it matters a lot (same axis range on x and y) and when it is more of a personal preference. Background grids** Discusses when and how to use background grids and other guide lines in figures. Don’t box yourself in Argues to avoid boxes and frames around figure parts. Avoid line drawings** Argues that filled shapes and solid colors are almost always preferable to shapes shown as outlines or with hatching or cross-hatching. The principle of proportional ink Explains that the size of colored areas needs to be proportional to the data value they represent. Handling overlapping points* Describes different strategies to handle the problems of overlapping points or large point clouds. These problems frequently arise in large datasets, and helpful strategies include using partially transparent points, 2d density plots, hex grids, or smoothers. Multi-part figures Discusses issues that arise in multi-part figures, including proper labeling, alignment between subfigures, shared legends, and overly complex multi-part figures. Don’t go 3d Argues why 3d plots are generally problematic (figures are fundamentally a 2d medium, and in 3d plots data is subjected to an additional, non-obvious transformation from 3d to 2d) and suggests alternatives to visualize high-dimensional datasets, including encoding additional variables in color, size, or symbol shape, and/or using faceting. Part II: A visualization for every occasion Directory of visualizations* Provides a graphical guide to the most commonly used types of data visualizations, with pointers to relevant other chapters in the book. Visualizing amounts Discusses bar plots in various forms. Visualizing proportions** Discusses stacked bar plots, stacked density plots, and pie charts. Visualizing paired data Discusses common strategies for paired data, including scatter plots and paired dot plots. Visualizing time series Discusses common strategies for time series, including line plots and sparklines Visualizing trends Discusses various approaches to smoothing data (linear regression line, GAMs, splines), and common pitfalls (many smoothers are unreliable or misleading at the edges of the data range). Visualizing distributions I: Histograms and density plots** Discusses strategies for visualizing individual distributions, including pros and cons of histograms and density plots. Visualizing distributions II: Cumulative density functions and q-q plots** Discusses strategies for visualizing distributions that are exact and non-arbitrary (unlike histograms and density plots) but are more difficult to interpret. Visualizing distributions III: Boxplots, violin plots, and more* Discusses strategies for visualizing many distributions, including boxplots, violin plots, jittered points, and others. Visualizing arrays of intensities Discusses heat maps. Part III: Miscellaneous topics Understanding the most commonly used image file formats** Provides an introduction to vector and bitmap graphics and describes the pros and cons of the various most commonly used file formats. Choosing the right plotting software Discusses the pros and cons of different software available to make graphs. Selecting figures for a report, paper, or presentation Discusses how to compile larger sets of figures to tell a story; e.g., always move from less processed to more processed data representations; also, avoid repeating the same type of figure many times. Annotated bibliography Provides a list of other reading material on related topics, with a brief paragraph describing the contents of each reference. 14.2 Other notes and comments Articles and blog posts with useful ideas: https://medium.com/@clmentviguier/how-to-turn-a-twitter-comment-into-a-data-visualisation-design-opportunity-7447402f0c2f http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html http://socviz.co/ Births per month visualization: https://github.com/aaronpenne/Data_Visualization/blob/master/HMD/Births_USA/Births_USA.ipynb http://www.datavizcatalogue.com/about.html Data acquisition (four parts): https://ikashnitsky.github.io/2017/data-acquisition-one/ https://ikashnitsky.github.io/2017/data-acquisition-two/ https://ikashnitsky.github.io/2017/data-acquisition-three/ Datasets to use: midwest (ggplot2) overlapping points: mpg cty vs hwy economics (ggplot2) gapminder (gapminder) gss_sm (socviz) http://gss.norc.org/Get-Documentation organdata (socviz) titanic_test, titanic_train (titanic) (is a better source of this dataset available?) various datasets of openintro package, https://cran.r-project.org/web/packages/openintro/index.html Moby Dick wordcounts, downloaded from http://tuvalu.santafe.edu/~aaronc/powerlaws/data.htm, reference: M. E. J. Newman, “Power laws, Pareto distributions and Zipf’s law.” Contemporary Physics 46, 323 (2005). Berkeley list of health statistics datasets: http://guides.lib.berkeley.edu/publichealth/healthstatistics/rawdata Birth and death rates: http://www.mortality.org/cgi-bin/hmd/hmd_download.php OECD, http://stats.oecd.org/, e.g. tax as % of GDP Required versions of rendering software: pandoc: 1.19.2.1 rmarkdown: 1.8 bookdown: 0.4 "],
["references.html", "References", " References "]
]
