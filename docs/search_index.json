[
["index.html", "Fundamentals of data visualization Preface", " Fundamentals of data visualization Claus O. Wilke Preface This book is meant as a guide for making figures that look professional and are publication-ready. It has grown out of my experience of having to repeatedly give my trainees the same kinds of advice—use larger fonts, pay attention to the aspect ratio of your figure, use solid colors rather than outlines, and so on. Now, I can just aks them to read the appropriate chapters in this book. The entire book was written in R Markdown, using RStudio as my text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub, at https://github.com/clauswilke/dataviz. If you would like to fix typos or other issues, feel free to send me pull requests through GitHub. In your commit message, please add the sentence “I assign the copyright of this contribution to Claus O. Wilke,” so that I can maintain the option of publishing this book in other forms. For comments, questions, or requests for additional chapters, please open an issue on GitHub. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. "],
["introduction.html", "Introduction Thoughts on graphing software and figure-preparation pipelines Good, bad, and ugly figures Acknowledgments", " Introduction If you are a scientist, an analyst, a consultant, or anybody else who has to prepare technical documents or reports, one of the most important skills you need to have is the ability to make compelling data visualizations, generally in the form of figures. Figures will typically carry the weight of your arguments. They need to be clear, attractive, and convincing. The difference between good and bad figures can be the difference between a highly influential or a obscure paper, a grant or contract won or lost, a job interview gone well or poorly. And yet, there are surprisingly few resources to teach you how to make compelling data visualizations. There are no college courses on this topic, and no extensive collection of books you can read.1 Tutorials for plotting sofware typically focus on how to achieve specific visual effects rather than explaining why certain choices are preferred and others not. In your day-to-day work, you are simply expected to know how to make good figures, and if you’re lucky you have a patient adviser who teaches you a few tricks as you’re writing your first scientific papers. In the context of writing, experienced editors talk about “ear”, the ability to hear (internally, as you read a piece of prose) whether the writing is any good. I think that when it comes to figures and other visualizations, we similarly need “eye”, the ability to look at a figure and see whether it is balanced, clear, and compelling. And just as is the case with writing, the ability to see whether a figure works or not can be learned. Having eye means primarily that you are aware of a larger collection of simple rules and principles of good visualization, and that you pay attention to little details that other people might not. In my experience, again just as in writing, you don’t develop eye by reading a book over the weekend. It is a lifelong process, and concepts that are too complex or too subtle for you today may make much more sense five years from now. I can say for myself that I continue to evolve in my understanding of figure preparation. I routinely try to expose myself to new approaches, and I pay attention to the visual and design choices others make in their figures. I’m also open to change my mind. I might today consider a given figure great, but next month I might find a reason to criticize it. So with this in mind, please don’t take anything I say as gospel. Think critically about my reasoning for certain choices and decide whether you want to adopt them or not. This book is written as a series of independent blog posts, and there is no need to read it cover to cover. Feel free to skip around, to pick out a specific section that you’re interested in at the moment, or one that covers a specific design choice you’re pondering. In fact, I think you will get the most out of this book if you don’t read it all at once, but rather read it piecemeal over longer stretches of time, try to apply just a few concepts from the book in your figuremaking, and come back to read about other concepts or re-read concepts you learned about a while back. You may find that the same chapter tells you different things if you re-read it after a few months of time have passed. Even though all the figures in this book were made with R and ggplot2, I do not see this as an R book. I am talking about general principles of figure preparation. The software used to make the figures is incidental. You can use any plotting software you want to generate the kinds of figures I’m showing here, even if ggplot2 and similar packages make many of the techniques I’m using much simpler than other plotting libraries. Importantly, because this is not an R book, I do not discuss code or programming techniques anywhere in this book. I want you to focus on the concepts and the figures, not on the code. If you are curious how any of the figures were made, you can check out the book’s source code at its GitHub repository, https://github.com/clauswilke/professional_figures. Thoughts on graphing software and figure-preparation pipelines I have over two decades of experience preparing figures for scientific publications and have made thousands of figures. If there is one constant over these two decades, it’s the change in figure preparation pipelines. Every few years, a new plotting library is developed or a new paradigm arises, and large groups of scientists switch over to the hot new toolkit. I have made figures using gnuplot, Xfig, Mathematica, Matlab, matplotlib in python, base R, ggplot2 in R, and possibly others I can’t currently remember. My current preferred approach is ggplot2 in R, but I’m under no illusion that I’ll continue using it until I retire. This constant change in software platforms is one of the key reasons why this book is not a programming book and why I have left out all code examples. I want this book to be useful to you regardless of which software you use, and I want it to remain valuable even once everybody has moved on from ggplot2 and uses the next new thing. I realize that this choice may be frustrating to some ggplot2 uses who would like to know how I made a given figure. To them I say, read the source code of the book. It is available. Also, in the future I may release a supplementary document focused just on the code. One thing I have learned over the years is that automation is your friend. I think figures should be autogenerated as part of the data analysis pipeline (which should also be automated), and they should come out of the pipeline ready to be sent to the printer, no manual post-processing needed. I see a lot of trainees autogenerate rough drafts of their figures, which they then import into Illustrator for sprucing up. There are several reasons why this is a bad idea. First, the moment you manually edit a figure, your final figure becomes irreproducible. A third party cannot generate the exact same figure you did. While this may not matter much if all you did was change the font of the axis labels, the lines are blurry, and it’s easy to cross over into territory where things are less clear cut. As an example, let’s say to manually replaced cryptic labels with more readable ones. A third party may not be able to verify that the label replacement was appropriate. Second, if you add a lot of manual post-processing to your figure-preparation pipeline then you will be more reluctant to make any changes or redo your work. Thus, you may ignore reasonable requests for change made by collaborators or colleagues, or you may be tempted to re-use an old figure even though you actually regenerated all the data. These are not made-up examples. I’ve seen all of them play out with real people and real papers. Third, you may yourself forget what exactly you did to prepare a given figure, or you may not be able to generate a future figure on new data that exactly visually matches your earlier figure. For all the above reasons, interactive plot programs are a bad idea. They inherently force you to manually prepare your figures. In fact, it’s probably better to auto-generate a figure draft and spruce it up in Illustrator than make the entire figure by hand in some interactive plot program. Please be aware that Excel is an interactive plot program as well and is not recommended for figure preparation (or data analysis). One critical component in a book on data visualization is feasibility of the proposed visualizations. It’s nice to invent some elegant new way of visualization, but if nobody can easily generate figures using this visualization then there isn’t much use to it. For example, when Tufte first proposed sparklines nobody had an easy way of making them. While we need visionaries who move the world foward by pushing the envelope of what’s possible, I envision this book to be practical and directly applicable to working scientists preparing figures for their publications. Therefore, every visualization I propose in the subsequent chapters can be generated with a few lines of R code via ggplot2 and readily available extension packages. In fact, every figure in this book was autogenerated exactly as shown, using R and ggplot2. Good, bad, and ugly figures Throughout this book, I am showing many different versions of the same figures, some as examples of how make a good visualization and some as examples of how not to. To provide a simple visual guideline of which examples should be emulated and which should be avoided, I am grading each figure on a three-point scale, “good”, “ugly”, and “bad”: good—A figure that looks nice and could be printed as is. Note that among good figures, there will still be differences in quality, and some good figures will be better than others. ugly—A figure that has one ore more shortcomings that should be remediated, or a figure that I personally don’t find aesthetically pleasing. bad—A figure that has one or more objective flaws which make it unsuitable for publication. I generally provide my rationale for specific ratings, but some are a matter of taste. In particular when it comes to the difference between “ugly” and “good”, reasonable people may disagree. I encourage you to develop your own eye and to critically evaluate my choices. Acknowledgments This project would not have been possible without the fantastic work the RStudio team has put into turning the R universe into a first-rate publishing platform. In particular, I have to thank Hadley Wickham for creating ggplot2, the plotting software that was used to make all the figures throughout this book. I would also like to thank Yihui Xie for creating the knitr package, R Markdown, and bookdown. I don’t think I would have started this project without these tools ready to go. Even though I have over 20 years of experience authoring documents in LaTeX, and I have written and continue to write almost all my scientific papers using that platform, the idea of writing a LaTeX document that contains hundreds of figures, having to maintain the R code for all these figures separately, and having to manually organize storage and naming of all the figure files makes me cringe. By contrast, writing self-contained R Markdown files is fun, and it’s easy to collect material and gain momentum. One notable exception is the works of Edward Tufte. His books and seminars are excellent, and much of what I do and say has been inspired by him.↩ "],
["visualizing-data-mapping-data-onto-aesthetics.html", "1 Visualizing data: mapping data onto aesthetics", " 1 Visualizing data: mapping data onto aesthetics "],
["choosing-colorblind-friendly-color-scales.html", "2 Choosing colorblind-friendly color scales 2.1 Qualitative color scales 2.2 Directional color scales", " 2 Choosing colorblind-friendly color scales When coloring elements in your figures, you need keep in mind that a good proportion of your readers may have color-vision deficiency (i.e., are colorblind) and may not be able to distinguish the colors that look so clearly different to you. You could address this problem by preparing all figures in grayscale, but those figures would look drab and boring to everybody, even those with impaired color vision. Alternatively, you can (and should) employ redundant coding, discussed in more detail in Chapter 3. However, independent of whether or not you use redundant coding, you should consider using a color scale whose colors are distinguishable even for people with color-vision deficiency. In choosing your color scale, remember that people with impaired color vision are not literally unable to see any colors. Instead, they will typically have difficulty to distinguish certain types of colors, for example red and green (red–green colorblindness) or blue and green (blue-yellow colorblindness). The key to making a colorblind-friendly color scale is to (i) choose colors with different levels of brightness and saturation, and (ii) choose color combinations that tend to look dissimilar even if color vision is partially impaired. Unless you’re an expert in color theory, go with an existing color scheme instead of designing your own. 2.1 Qualitative color scales We frequently need to color discrete items that do not have an inherent order, such as different countries on a map or different manufacturers of a certain product. In those cases, we use qualitative color scales, which are color scales with a finite set of specific colors that are chosen to look as different from each other as possible. My preferred color scale of this type, which I use extensively throughout this book, was developed specifically to work well for all the major types of color-vision deficiency (Okabe and Ito 2008): By providing eight different colors, this palette works for nearly any scenario with discrete colors. You should probably not color-code more than eight different items in a plot anyways. A variant of the palette replaces black with gray, if you don’t like to see completely black visual elements: In these palettes, the alphanumeric codes represent the colors in RGB space, encoded as hexadecimals. In many plot libraries and image-manipulation programs, you can just enter these codes directly. If your software does not take hexadecimals directly, you can also use Table 2.1. Table 2.1: Colorblind-friendly color scale, developed by Okabe and Ito (2008). Name Hex code Hue C, M, Y, K (%) R, G, B (0-255) R, G, B (%) black #000000 - 0, 0, 0, 100 0, 0, 0 0, 0, 0 gray #999999 - 0, 0, 0, 60 153, 153, 153 60, 60, 60 orange #E69F00 41° 0, 50, 100, 0 230, 159, 0 90, 60, 0 sky blue #56B4E9 202° 80, 0, 0, 0 86, 180, 233 35, 70, 90 bluish green #009E73 164° 97, 0, 75, 0 0, 158, 115 0, 60, 50 yellow #F0E442 56° 10, 5, 90, 0 240, 228, 66 95, 90, 25 blue #0072B2 202° 100, 50, 0, 0 0, 114, 178 0, 45, 70 vermilion #D55E00 27° 0, 80, 100, 0 213, 94, 0 80, 40, 0 reddish purple #CC79A7 326° 10, 70, 0, 0 204, 121, 167 80, 60, 70 2.2 Directional color scales A second common use of color is to indicate intensity of some sort, for example temperature or speed. In those cases, we need a directional color scale that clearly indicates (i) which values are larger or smaller than which other ones, and (ii) how distant two specific values are from each other. The second point is particularly important, because it implies that the color scale needs to vary uniformly across its entire range. There cannot be parts of the scale where color changes more quickly than in other parts. Designing such a color scale, in particular while respecting the additional requirement that they work for viewers with impaired color vision, can be surprisingly difficult. However, recent research into color scales has solved these issues and produced some very nice color scales (Smith and van der Walt 2015). In brief, all these color scales satisfy the following conditions. First, brightness varies continuosly from dark to light, such that the color scale works even when reproduced in grayscale. Second, the major color axis changes from blue to yellow, because the alternative, red to green, would not work for the most common forms of color-vision deficiency, red-green blindness. Third, the blue colors appear on the dark side of the scale and the yellow colors on the light side, because gradients from dark yellow to bright blue look unnatural. With these constraints, there is only one major choice remaining, which is whether to transition from blue to yellow through green or through red. The following four color scales, which are now available for both python and R graphics and are becoming increasingly popular, meet all these requirements. You can see examples using these scales throughout this book, for example in the chapter on redundant coding (Chapter 3). ## Loading required package: ggplot2 ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union ## Loading required package: methods ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## stamp ## The following object is masked from &#39;package:base&#39;: ## ## date ## Loading required package: ggridges ## The ggjoy package has been deprecated. Please switch over to the ## ggridges package, which provides the same functionality. Porting ## guidelines can be found here: ## https://github.com/clauswilke/ggjoy/blob/master/README.md ## Loading required package: viridisLite References "],
["redundant-coding.html", "3 Redundant coding", " 3 Redundant coding Need some intro text here about the concept of redundant coding, showing the same information with multiple visua elements that reinforce each other. There are two problems with this figure: 1. All points have the same shape. 2. The two colors that are more similar to each other (blue and green) are used for the data points that are the most intermingled (virginica and versicolor). We address these two problems by using different shapes and switching out the colors. Notice how it has become easier to distinguish virginica and versicolor. Also, now that we use different shapes, we see that some virginica points fully overlap with some versicolor points. (These points appear as little stars, because the square representing versicolor is shown underneath the diamond representing virginica.) To learn more about ways to handle overlapping points, see Chapter 6. I have removed the background grid from this figure because otherwise the figure was becoming too busy. Whenever possible, design your figures so they don’t need a legend. Let’s consider another example that also demonstrates a very common visualization mistake. The figure contains four lines, representing the stock prices of four different companies. The lines are color coded using a colorblind-friendly color scale. So it should be relatively straightfoward to associate each line with the corresponding company. Yet it is not. The problem here is that the data lines have a clear visual order. The yellow line, representing Facebook, is clearly the highest line, and the black line, representing Apple, is clearly the lowest, with Alphabet and Microsoft inbetween, in that order. Yet the order of the four companies in the legend is Alphabet, Apple, Facebook, Microsoft (alphabetic order). Thus, the perceived order of the data lines differs from the order of the companies in the legend, and it takes a surprising amount of effort to match data lines with company names. This problem arises commonly with plotting software that autogenerates legends, as is the case for instance with R’s ggplot2. The plotting software has no concept of the visual order that the viewer will perceive in the data, and it will instead sort the legend by some other order, most commonly alphabetical. We can fix this problem by manually reordering the entries in the legend so they match the preceived ordering in the data. While the above figure is a major improvement, we can still do better. Even with the correct ordering, a legend imposes an unnecessary mental burden on the viewer of the figure. The viewer has to match the colors in the legend to the colors in the figure and translate which company name goes with which data line. It is better to get rid of the legend altogether and instead draw the company names right next to the data lines. I’ll provide a few more examples. First, in a density plot we may want to label the density curves directly rather than adding a color legend. Second, we can also combine this concept with the scatter plot from the beginning of this chapter to avoid a legend in that plot. And finally, here is an example where the color bar (representing temperature) is integrated into the temperature axis. "],
["small-axis-labels.html", "4 Your axis labels are too small", " 4 Your axis labels are too small If you take away only one single lesson from this book, make it this one: Pay attention to your axis labels, axis tick labels, and other assorted plot annotations. Chances are they are too small. In my experience, nearly all plot libraries and graphing softwares have poor defaults. If you use the default values, you’re almost certainly making a poor choice. For example, consider the following figure. I see figures like this all the time. The axis labels, axis tick labels, and legend labels are all incredibly small. We can barely see them, and we may have to zoom into the page to distinguish FWD from RWD in the figure legend. A somewhat better version of this figure would be this one: I think the fonts are still too small, and that’s why I have labeled it “ugly”. However, we are moving in the right direction, and this figure might be passable under some circumstances. My main criticism with this figure is not so much that the labels aren’t legible as that it is not balanced; the text elements are too small compared to the rest of the figure. The next figure uses the default settings I’m applying throughout this book. I think it is well balanced, the text is clearly visible, and it fits with the overall size of the figure. Importantly, we can overdo it and make the labels too big: Sometimes we need big labels, in particular if the figure is meant to be reduced in size, but the various elements of the figure (in particular, label text and plot symbols) need to fit together. In the above example, the points used to visualize the data are too small relative to the text. Once we fix this, the figure becomes acceptable again: You may look at this figure and find everything too big. However, keep in mind that it is meant to be scaled down. Scale it down so that it is only an inch or two in width, and the figure looks just fine. In fact, at that scaling this is the only figure in this chapter that looks good. Always look at scaled-down versions of your figures to make sure the axis labels are appropriately sized. I think there is a simple psychological reason for why we routinely make figures whose axis labels are too small, and it relates to large, high-resolution computer monitors. We routinely preview figures on the computer screen, and often we do so while the figure takes up a large amount of space on the screen. In this viewing mode, even comparatively small text seems perfectly fine and legible, and large text can seem awkward and overpowering. In fact, if you take the first figure from this chapter and magnify it to the point where it fills your entire screen, you will likely think that it looks just fine. The solution is to always make sure that you look at your figures at a realistic print size. You can either zoom out so they are only three to five inches in width on your screen, or you can go to the other side of your room and check whether the figure still looks good from a substantial distance. "],
["background-grids.html", "5 Background grids", " 5 Background grids With the rising popularity of the R package ggplot2, which uses a gray background grid as default, graphs with this style have become widespread. With apologies to Hadley Wickham, for whom I have the utmost respect, I don’t find this style particularly attractive. In general, I find that the gray background detracts from the actual data. As an example, consider this figure, which shows the stock price of four major tech companies, indexed to their value in June 2012: The grid is too busy, and the gray background in the legend is distracting. We could try to remove the grid altogether, but I think that is a worse option: Now the curves seem to just float in space, and it’s difficult to see where they go. In addition, since all prices are indexed to 100 in June 2012, at a minimum this value should be marked in the plot. Thus, one option would be to add a thin horizontal line at \\(y=100\\): Alternatively, we can use just a minimal grid. In particular, for a plot where we are primarily interested in the change in \\(y\\) values, vertical grid lines are not needed. Moreover, grid lines positioned at only the major axis ticks will often be sufficient. And, the axis line can be omitted or made very thin. Thus, we arrive at this plot: Note that for such a minimal grid, we generally draw the lines orthogonally to direction along which the numbers of interest vary. Thus, if instead of plotting the stock price over time we plot the five-year increase, as horizontal bars, then we will want to use vertical lines instead: Note that in addition to the light vertical lines, I have also added the actual numerical values that each bar represents. Whenever your plot is meant to display only a small number of key values (like here, four), it makes sense to add the actual numbers to the plot. This substantially increases the amount of information conveyed by your plot without adding much visual noise. Background grids along both axis directions can make sense for scatter plots where there is no primary axis of interest, in particular if you leave out the axis lines: For figures where the relevant comparison is the \\(x=y\\) line, I prefer to draw a diagonal line rather than a grid. For example, consider the following figure, adapted from Echave, Spielman, and Wilke (2016), which compares two sets of correlations for 209 protein structures. By drawing the diagonal line, we can see immediately which correlations are systematically stronger: The same observation is much harder to make when the figure has a background grid instead. Thus, even though this figure looks pleasing, I label it as “bad”: References "],
["overlapping-points.html", "6 Handling overlapping points 6.1 Partial transparency 6.2 Jittering 6.3 Contour lines 6.4 2d histograms", " 6 Handling overlapping points When we want to visualize large or very large datasets, we often experience the challenge that simple x–y scatter plots do not work very well because many points lie on top of each other and partially or fully overlap. And similar problems can arise even in small datasets if values were recorded with low precision or rounded, such that multiple observations have exactly the same numeric values. The technical term commonly used to describe this situation is “overplotting”, i.e., plotting many points on top of each other. Below, I describe several strategies you can pursue when you encounter this challenge. 6.1 Partial transparency Let’s revisit the fuel economy figure from Chapter 4 and focus on an aspect we ignored there: I have labeled this figure “bad” here because the points overlap and partly obscure each other. A simple way to ameliorate this issue is to use partial transparency: 6.2 Jittering Making points partially transparent is not always sufficient. For example, the same dataset contains fuel economy for both city and highway driving. If we plot those two quantities against each other, we obtain the following figure. Because fuel economy is rounded to whole integers in this dataset, many points lie exactly on top of each other. While these fully overlapping points appear darker in the plot, the visual appearance is that of one darker point rather than of a set of points plotted in the same location. We can emphasize the number of points in the same locations by applying a small amount of jitter, i.e., displacing each point randomly by a small amount. However, when jittering we have to make sure not to overdo it. If we jitter too much, we end up placing points in locations that are not representative of the underlying dataset and hence are creating a misleading visualization of the data. For example, in this particular case the extreme jittering creates the impression that for some cars the highway economy can fall below the city economy. However, such cases do not exist in the original dataset. 6.3 Contour lines When the number of points grows large, it can be helpful to indicate the point density, for example with contour lines. This technique works well for the following figure, which shows the total population as a function of area for counties in the midwest. If we want to emphasize the overall features of the distribution rather than the individual points, we can also show only the contour lines and leave out the individual points. Finally, we can add a smoothing line to highlight the overall trend in the relationship between the two variables. 6.4 2d histograms None of the techniques discussed so far work very well when the majority of points falls into a small area relative to the overall extent of the data and the overall extent highlights important data features. Consider the following figure, which shows the departure delay in minutes versus the flight departure time, for all flights departing Newark airport (EWR) in 2013. Even though we have made the points fairly transparent, the majority of the points just form a black band between 0 and 300 minutes departure delay. This black band obscures whether most flights depart approximately on time or with substantial delay (say 50 minutes or more). At the same time, the most delayed flights (with delays of 400 minutes or more) are barely visible due to the transparency of the points. A good solution for this particular case is a 2d histogram, where we subdivide the entire x–y plane into small squares, count how many observations fall into each square, and then color the square by that count. The result is the following figure. This figure clearly highlights several important features. First, the vast majority of departures during the day (6am to about 9pm) actually depart without delay or even early (negative delay). However, a modest number of departures has a substantial delay. Moreover, the later a plane departs in the day the more of a delay it can have. Importantly, here the departure time is the actual time of departure, not the scheduled time of departure. So this figure does not necessarily tell us that planes scheduled to depart early never experience delay. What it does tell us, though, is that if a plane departs early it either has little delay or, in very rare cases, a delay of around 900 minutes. As an alternative to binning the data into squares, we can also bin into hexagons. This approach, first proposed by Carr et al. (1987), has the advantage that the points in a hexagon are, on average, closer to the hexagon center than the points in an equal-area square are to the center of the square. As a consequence, the colored hexagon represents the data slightly more accurately than the colored square does. The following figure shows the same data with hexagon binning rather than square binning. References "],
["directory-of-visualizations.html", "7 Directory of visualizations", " 7 Directory of visualizations "],
["visualizing-distributions.html", "8 Visualizing distributions", " 8 Visualizing distributions We frequently encounter the situation where we would like to understand how a particular variable is distributed in a dataset. To give a concrete example, consider the passengers of the ship Titanic, which sank on April 15, 2012. There were approximately 1317 passengers on the Titanic (not counting crew), and we have reported ages for 756 of them. We might want to know how many passengers of what ages there were on the Titanic, i.e., how many children, young adults, middle-aged people, seniors, and so on. We call the relative proportions of different ages among the passengers the age distribution of the passengers. We can obtain a sense of the age distribution among the passengers by grouping all passengers into bins with comparable ages and then counting the number of passengers in each bin. This procedure results in a table such as Table 8.1. Table 8.1: Numbers of passenger with known age on the Titanic. age range count 0–5 36 6–10 19 11–15 18 16–20 99 21–25 139 26–30 121 age range count 31–35 76 36–40 74 41–45 54 46–50 50 51–55 26 56–60 22 age range count 61–65 16 66–70 3 71–75 3 We can visualize this table by drawing filled rectangles whose heights correspond to the counts and whose widths correspond to the width of the age bins. Such a visualization is called a histogram. Discus histogram dependency on bin width. "],
["boxplots-violin-plots-and-more.html", "9 Boxplots, violin plots, and more", " 9 Boxplots, violin plots, and more We commonly have to visualize multiple distributions at the same time. For example, consider weather data. We have observations for each day in a month, possibly at multiple time points or multiple locations, but we frequently are interested in the broader trends, such as how temperature changes with month. The following figure visualizes temperature data collected in Lincoln, Nebraska in 2016. The dataset contains the mean temperature for each day of the year. We could plot this dataset by calculating the average mean temperature in each month and plotting it as points with error bars. However, there are multiple problems with this approach. First, we’re losing a lot of information about the data. Second, it’s not necessarily clear what the points represent. Third, it’s definitely not clear what the errorbars represent. There is no standard. Do they represent the standard deviation of the data, the standard error of the mean, a 95% confidence interval, or something else altogether? (I’m here plotting twice the standard deviation, to indicate the range that contains approximately 95% of the data.) Fourth, symmetric error bars are misleading if there is any skew in the data, which is the case here and almost always for real-world datasets. A traditional and commonly used method of visualizing key parameters of distributions is the boxplot. The boxplot divides the data into quartiles and visualizes them in a standardized manner. The line in the middle represents the median, and the box encloses the middle 50% of the data. The top and bottom wiskers extend either to the maximum and minimum of the data, respectively, or to 1.5 times the height of the box, whichever yields the shorter wisker. When the wiskers extend to 1.5 times the height of the box, they are called the upper and lower fence, respectively. Individual data points that fall beyond the upper or lower fence are referred to as outliers and usually showns as individual dots. When we visualize the temperature dataset using boxplots, we obtain the following result. Using the boxplot visualization, we see clearly that temperature is highly skewed in December (most days are moderately cold, and a few are extremely cold) and not very skewed at all in some other months, e.g., July. We can also plot all individual points: Finally, we can combine the best of both worlds and spread the dots out in proportion to the number of points with a similar y coordinate. This methods yields the sina plot, which shows each individual dot while also visualizing the distributions. "],
["understanding-the-most-commonly-used-image-file-formats.html", "10 Understanding the most commonly used image file formats 10.1 Bitmap and vector graphics 10.2 Lossless and lossy compression of bitmap graphics 10.3 Converting between image formats", " 10 Understanding the most commonly used image file formats Anybody who is making figures for data visualization will eventually have to know a few things about how figures are stored on the computer. There are many different image file formats, and each has its own set of benefits and disadvantages. Choosing the right file format and the right workflow can alleviate many figure-preparation headaches. My own preference is to use pdf for high-quality publication-ready files and generally whenever possible, png for online documents and other scenarios where bitmap graphics are required, and jpeg as the final resort if the png files are too large. In the following, I explain the key differences between these file formats and their respective benefits and drawbacks. 10.1 Bitmap and vector graphics The most important difference between the various graphics formats is whether they are bitmap or vector (Table 10.1). Bitmaps or raster graphics store the image as a grid of individual points (called pixels), each with a specified color. By contrast, vector graphics store the geometric arrangement of individual graphical elements in the image. Thus, a vector image contains information such as “there’s a black line from the top left corner to the bottom right corner, and a red line from the bottom left corner to the top right corner,” and the actual image is recreated on the fly as it is displayed on screen or printed. Table 10.1: Commonly used image file formats Acronym Name Type Application pdf Portable Document Format vector general purpose eps Encapsulated PostScript vector general purpose, outdated; use pdf svg Scalable Vector Graphics vector online use png Portable Network Graphics bitmap optimized for line drawings jpeg Joint Photographic Experts Group bitmap optimized for photographic images tiff Tagged Image File Format bitmap print production, accurate color reproduction raw Raw Image File bitmap digital photography, needs post-processing gif Graphics Interchange Format bitmap outdated, do not use Vector graphics are also called “resolution-independent,” because they can be magnified to arbitrary size without losing detail or sharpness. See Figure 10.1 for a demonstration. Figure 10.1: Illustration of the key difference between vector graphics and bitmaps. (a) Original image. The black square around the number seven indicates the area we’re magnifying in parts (b) and (c). (b) Increasing magnification of the highlighted area from part (a) when the image has been stored as a bitmap graphic. We can see how the image becomes increasingly pixelated and blurry as we zoom in further. (c) As part (b), but now for a vector representatin of the image. The image maintains perfect sharpness at arbitrary magnification levels. Vector graphics have two down-sides that can and often do cause trouble in real-world applications. First, because vector graphics are redrawn on the fly by the graphics program with which they are displayed, it can happen that there are differences in how the same graphic looks in two different programs, or on two different computers. This problem occurs most frequently with text, for example when the required font is not available and the rendering software substitutes a different font. Font substitutions will typically allow the viewer to read the text as intended, but the resulting image rarely looks good. There are ways to avoid these problems, such as outlining or embedding all fonts in a pdf file, but they may require special software and/or special technical knowledge to achieve. By contrast, bitmap images will always look correct. Second, for very large and/or complex figures, vector graphics can grow to enourmous file sizes and be slow to render. For example, a scatter plot of millions of data points will contain the x and y coordinates of every individual point, and each point needs to be drawn when the image is rendered, even if points overlap and/or are hidden by other graphical elements. As a consequence, the file may be many megabytes in size, and it may take the rendering software some time to display the figure. When I was a postdoc in the early 2000s, I once created a pdf file that at the time took almost an hour to display in the Acrobat reader. While modern computers are much faster and rendering times of many minutes are all but unheard of these days, even a rendering time of a few seconds can be disruptive if you want to embed your figure into a larger document and your pdf reader grinds to a halt every time you display the page with that one offending figure. Of course, on the flip side, simple figures with only a small number of elements (a few data points and some text, say) will often be much smaller as vector graphics than as bitmaps, and the viewing software may even render such figures faster than it would the corresponding bitmap images. 10.2 Lossless and lossy compression of bitmap graphics Most bitmap file formats employ some form of data compression to keep file sizes manageable. There are two fundamental types of compression: lossless and lossy. Lossless compression guarantees that the compressed image is pixel-for-pixel identical to the original image, whereas lossy compression accepts some image degradation in return for smaller file sizes. To understand which approach is appropriate when, it is helpful to have a basic understanding of how these different compression algorithms work. Let’s first consider lossless compression. Imagine an image with a black background, where large areas of the image are solid black and thus many black pixels appear right next to each other. Each black pixel can be represented by three zeroes in a row: 0 0 0, representing zero intensities in the red, green, and blue color channels of the image. The areas of black background in the image correspond to thousands of zeros in the image file. Now assume somewhere in the image are 1000 consecutive black pixels, corresponding to 3000 zeros. Instead of writing out all these zeros, we could store simply the total number of zeros we need, e.g. by writing 3000 0. In this way, we have conveyed the exact same information with only two numbers, the count (here, 3000) and the value (here, 0). Over the years, many clever tricks along these lines have been developed, and modern lossless image formats (such as png) can store bitmap data with impressive efficiency. However, all lossless compression algorithms perform best when images have large areas of uniform color, and therefore Table 10.1 lists png as optimized for line drawings. Photographic images rarely have multiple pixels of identical color and brightness right next to each other. Instead they have gradients and other somewhat regular patterns on many different scales. Therefore, lossless compression of these images often doesn’t work very well, and lossy compression has been developed as an alternative. The key idea of lossy compression is that some details in an image are too subtle for the human eye, and those can be discarded without obvious degradation in the image quality. For example, consider a gradient of 1000 pixels, each with a slightly different color value. Chances are the gradient will look nearly the same if it is drawn with only 200 different colors and coloring every five adjacent pixels in the exact same color. The most widely used lossy image format is jpeg (Table 10.1), and indeed many digital cameras output images as jpeg by default. Jpeg compression works exceptionally well for photographic images, and huge reductions in file-size can often be obtained with very little degradation in image quality. However, jpeg compression fails when images contain sharp edges, such as created by line drawings or by text. In those cases, jpeg compression can result in very noticeable artifacts (Figure 10.2). Figure 10.2: Illustration of jpeg artifacts. (a) The same image is reproduced multiple times using increasingly severe jpeg compression. The resulting file size is shown in the top-right corner of each image. A reduction in file size by a factor of 10, from 432kB in the original image to 43kB in the compressed image, results in only minor perceptible reduction in image quality. However, a further reduction in file size by a factor of 2, to a mere 25kB, leads to numerous visible artifacts. (b) Zooming in to the most highly compressed image reveals the various compression artifacts. Image credit: Claus O. Wilke 10.3 Converting between image formats It is generally possible to convert any image format into any other image format. For example, on a Mac, you can open an image with Preview and then export to a number of different formats. In this process, though, important information can get lost, and information is never regained. For example, after saving a vector graphic into a bitmap format, e.g. a pdf file as a jpeg, the resolution independence that is a key feature of the vector graphic has been lost. Conversely, saving a jpeg image into a pdf file does not magically turn the image into a vector graphic. The image will still be a bitmap image, just stored inside the pdf file. Similarly, converting a jpeg file into a png file does not remove any artifacts that may have been introduced by the jpeg compression algorithm. It is therefore a good rule of thumb to always store the original image in the format that maintains maximum resolution, accuracy, and flexibility. Thus, for data visualizations, create your figure as pdf and then convert into png or jpg when necessary. Similarly, for images that are only available as bitmaps, such as digital photographs, store them in a format that doesn’t use lossy compression, or if that can’t be done, compress as little as possible. Also, store the image in as high a resolution as possible, and down-scale when needed. "],
["notes.html", "11 Notes 11.1 Outline 11.2 Other notes and comments", " 11 Notes 11.1 Outline Chapters for which some material exists at this time are marked with stars, as follows: *Chapter started; made relevant figures and/or wrote rudimentary first draft **Chapter mostly complete Part I: General principles of figure design Visualizing data: mapping data onto aesthetics Explains the basic concept of aesthetic mapping, which lies at the heart of any data visualization. Figure titles Discusses when to use and not to use figure titles. For captioned figures, the titles are normally the first thing shown in the caption, and thus are not shown on top of the figure. Choosing colorblind-friendly color scales* Explains what color choices can and cannot be seen by people with colorblindness, and also introduces the three basic types of color scales: qualitative, directional, diverging. Material on diverging color scales is not yet written. Effective use of color in figures Covers basic concepts of color use, as a tool to highlight, as a tool to distinguish, and as a tool to represent a value. Redundant coding* Explains how to make sure that key information in the figure is provided in multiple, reduant ways, for example through color and location or color and direct labeling. Your axis labels are too small** Discusses the widespread problem of excessively small axis labels. Choosing the right axis settings Covers various aspects related to axis choice, including linear vs. logarithmic axes, as well as issues of axis expansion beyond the data range. Choosing an appropriate aspect ratio Discusses the concept of aspect ratio, including when it matters a lot (same axis range on x and y) and when it is more of a personal preference. Background grids** Discusses when and how to use background grids and other guide lines in figures. Don’t box yourself in Argues to avoid boxes and frames around figure parts. Avoid line drawings Argues that filled shapes and solid colors are almost always preferable to shapes shown as outlines or with hatching or cross-hatching. Handling overlapping points** Describes different strategies to handle the problems of overlapping points or large point clouds. These problems frequently arise in large datasets, and helpful strategies include using partially transparent points, 2d density plots, hex grids, or smoothers. Multi-part figures Discusses issues that arise in multi-part figures, including proper labeling, alignment between subfigures, shared legends, and overly complex multi-part figures. Don’t go 3d Argues why 3d plots are generally problematic (figures are fundamentally a 2d medium, and in 3d plots data is subjected to an additional, non-obvious transformation from 3d to 2d) and suggests alternatives to visualize high-dimensional datasets, including encoding additional variables in color, size, or symbol shape, and/or using faceting. Part II: A visualization for every occasion Directory of visualizations* Provides a graphical guide to the most commonly used types of data visualizations, with pointers to relevant other chapters in the book. Visualizing amounts Discusses bar plots in various forms. Visualizing proportions Discusses stacked bar plots, stacked density plots, and pie charts. Visualizing paired data Discusses common strategies for paired data, including scatter plots and paired dot plots. Visualizing time series Discusses common strategies for time series, including line plots and sparklines Visualizing trends Discusses various approaches to smoothing data (linear regression line, GAMs, splines), and common pitfalls (many smoothers are unreliable or misleading at the edges of the data range). Visualizing distributions I: Histograms and density plots Discusses strategies for visualizing individual distributions, including pros and cons of histograms and density plots. Visualizing distributions II: Boxplots, violin plots, and more** Discusses strategies for visualizing many distributions, including boxplots, violin plots, jittered points, and others. Visualizing distributions III: Joyplots Describes the joyplot, an effective tool for visualizing a very large number of related distributions. Visualizing arrays of intensities Discusses heat maps. Part III: Miscellaneous topics Understanding the most commonly used image file formats** Provides an introduction to vector and bitmap graphics and describes the pros and cons of the various most commonly used file formats. Choosing the right plotting software Discusses the pros and cons of different software available to make graphs. Selecting figures for a report, paper, or presentation Discusses how to compile larger sets of figures to tell a story; e.g., always move from less processed to more processed data representations; also, avoid repeating the same type of figure many times. Annotated bibliography Provides a list of other reading material on related topics, with a brief paragraph describing the contents of each reference. 11.2 Other notes and comments for paired data chapter, use protein correlation dataset from Echave et al? need a clear system of stamping figures while keeping some spacing. current system is ad-hoc and inconsistent. This touches all chapters already written. aspect ratio chapter needs to be completed in the colorblind package, add color simulations using the dichromat R package? this looks like a useful resource: dataviz catalogue the viridis package has several great color scales Articles and blog posts with useful ideas: https://medium.com/@clmentviguier/how-to-turn-a-twitter-comment-into-a-data-visualisation-design-opportunity-7447402f0c2f http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html http://socviz.co/ Datasets to use: midwest (ggplot2) overlapping points: mpg cty vs hwy economics (ggplot2) gapminder (gapminder) gss_sm (socviz) http://gss.norc.org/Get-Documentation organdata (socviz) titanic_test, titanic_train (titanic) (is a better source of this dataset available?) various datasets of openintro package, https://cran.r-project.org/web/packages/openintro/index.html "],
["references.html", "References", " References "]
]
